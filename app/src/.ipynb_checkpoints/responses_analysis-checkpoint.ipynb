{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Respalizer (c). Data analysis.\n",
    "## Made by Ilya Zakharkin (github.com/izaharkin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit of natural language processing (sentiment analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data: Given data - responses of people on different bank companies (two features - sentiment (text, in russian) and mark (integer, 1 <= mark <= 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: We must predict which mark will this person give consider his response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful materials:\n",
    "- scikit-learn example: http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py\n",
    "- article for beginners: https://habrahabr.ru/company/mlclass/blog/270591/\n",
    "- TJ texts classification: https://habrahabr.ru/post/327072/\n",
    "- MIPT NLP course: https://github.com/canorbal/NLP_MIPT/\n",
    "- Russian language processing research: http://corpus.leeds.ac.uk/mocky/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let`s have a look at the data - how much data do we have, and in what format is it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/responses_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28916"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mark</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Я имею кредитную карту. Пользуюсь ею длительно...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Всем привет! Я в этом банке , как только рухну...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Добрый вечер.Был вашим вкладчиком на протяжени...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Очень разочарована банком ВТБ24. За смс уведом...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Отвратительный банк, и обслуживание. 24.11.201...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mark                                        description\n",
       "0     5  Я имею кредитную карту. Пользуюсь ею длительно...\n",
       "1     5  Всем привет! Я в этом банке , как только рухну...\n",
       "2     1  Добрый вечер.Был вашим вкладчиком на протяжени...\n",
       "3     1  Очень разочарована банком ВТБ24. За смс уведом...\n",
       "4     1  Отвратительный банк, и обслуживание. 24.11.201..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let`s change 'description' to 'sentiment':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_columns = data.columns.values\n",
    "new_columns[1] = 'sentiment'\n",
    "data.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28916.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.081685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.591598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mark\n",
       "count  28916.000000\n",
       "mean       2.081685\n",
       "std        1.591598\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        1.000000\n",
       "75%        3.000000\n",
       "max        5.000000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 29k responses, no blank fields, so it is data without holes in it, sentiments are in russian language, marks are from 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    17651\n",
       "5     5749\n",
       "2     3391\n",
       "3     1484\n",
       "4      641\n",
       "Name: mark, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.mark.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be critical for classifier to have such a large number of one class and a little number of other classes, so we remember it and may do oversampling later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can try different approaches: classification with 5 classes, or regression with target 'mark'.\n",
    "### Let`s try classification first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before classification or regression we must extract features from the data, I`ll use **TfidfVectorizer** (because usually it is better than regular *bag-of-words*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.95 s, sys: 23.3 ms, total: 4.97 s\n",
      "Wall time: 4.97 s\n",
      "(28916, 134520)\n"
     ]
    }
   ],
   "source": [
    "X = data['sentiment']\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
    "\n",
    "%time X = vectorizer.fit_transform(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a data to train our models. Let`s try all the classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1). Linear models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data['mark']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01 \t accuracy=0.6105 +-0.0002\n",
      "C=0.1 \t accuracy=0.7511 +-0.0035\n",
      "C=0.5 \t accuracy=0.7799 +-0.0028\n",
      "C=1 \t accuracy=0.7844 +-0.0025\n",
      "C=5 \t accuracy=0.7832 +-0.0009\n",
      "C=10 \t accuracy=0.7791 +-0.0012\n",
      "C=100 \t accuracy=0.7643 +-0.0014\n",
      "C=200 \t accuracy=0.7612 +-0.0009\n",
      "C=500 \t accuracy=0.7572 +-0.0013\n",
      "C=1000 \t accuracy=0.7556 +-0.0015\n",
      "C=10000 \t accuracy=0.7504 +-0.0007\n",
      "C=15000 \t accuracy=0.7504 +-0.0010\n",
      "C=20000 \t accuracy=0.7501 +-0.0007\n",
      "C=100000 \t accuracy=0.7485 +-0.0021\n",
      "CPU times: user 10min 5s, sys: 547 ms, total: 10min 6s\n",
      "Wall time: 10min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_C = -1\n",
    "for cur_C in [0.01, 0.1, 0.5, 1, 5, 10, 100, 200, 500, 1000, 10000, 15000, 20000, 100000]:\n",
    "    cls = LogisticRegression(C=cur_C)\n",
    "    cv_scores = cross_val_score(cls, X, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_C = cur_C\n",
    "    print('C={0}'.format(cur_C), '\\t', \n",
    "          'accuracy={0:0.4f}'.format(cur_avg_acc), \n",
    "          '+-{0:0.4f}'.format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are: C=1 with accuracy=0.7844\n"
     ]
    }
   ],
   "source": [
    "print('Best params are: C={0} with accuracy={1:0.4f}'.format(best_C, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StochasticGradientDescentClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.0001, power_t=0.1 \t accuracy=0.7874 +-0.0024\n",
      "alpha=0.0001, power_t=0.5 \t accuracy=0.7871 +-0.0028\n",
      "alpha=0.0001, power_t=1 \t accuracy=0.7874 +-0.0023\n",
      "alpha=0.0001, power_t=1.5 \t accuracy=0.7875 +-0.0025\n",
      "alpha=0.0001, power_t=2 \t accuracy=0.7872 +-0.0031\n",
      "alpha=0.0001, power_t=3 \t accuracy=0.7871 +-0.0032\n",
      "alpha=0.0001, power_t=5 \t accuracy=0.7873 +-0.0027\n",
      "alpha=0.0001, power_t=10 \t accuracy=0.7871 +-0.0030\n",
      "alpha=0.0001, power_t=100 \t accuracy=0.7873 +-0.0025\n",
      "alpha=0.0001, power_t=500 \t accuracy=0.7875 +-0.0021\n",
      "alpha=0.0001, power_t=1000 \t accuracy=0.7870 +-0.0024\n",
      "alpha=0.0005, power_t=0.1 \t accuracy=0.7706 +-0.0033\n",
      "alpha=0.0005, power_t=0.5 \t accuracy=0.7703 +-0.0031\n",
      "alpha=0.0005, power_t=1 \t accuracy=0.7707 +-0.0034\n",
      "alpha=0.0005, power_t=1.5 \t accuracy=0.7708 +-0.0030\n",
      "alpha=0.0005, power_t=2 \t accuracy=0.7704 +-0.0028\n",
      "alpha=0.0005, power_t=3 \t accuracy=0.7711 +-0.0036\n",
      "alpha=0.0005, power_t=5 \t accuracy=0.7709 +-0.0032\n",
      "alpha=0.0005, power_t=10 \t accuracy=0.7703 +-0.0031\n",
      "alpha=0.0005, power_t=100 \t accuracy=0.7703 +-0.0027\n",
      "alpha=0.0005, power_t=500 \t accuracy=0.7716 +-0.0033\n",
      "alpha=0.0005, power_t=1000 \t accuracy=0.7704 +-0.0028\n",
      "alpha=0.001, power_t=0.1 \t accuracy=0.7512 +-0.0036\n",
      "alpha=0.001, power_t=0.5 \t accuracy=0.7513 +-0.0036\n",
      "alpha=0.001, power_t=1 \t accuracy=0.7508 +-0.0041\n",
      "alpha=0.001, power_t=1.5 \t accuracy=0.7508 +-0.0038\n",
      "alpha=0.001, power_t=2 \t accuracy=0.7509 +-0.0039\n",
      "alpha=0.001, power_t=3 \t accuracy=0.7509 +-0.0041\n",
      "alpha=0.001, power_t=5 \t accuracy=0.7508 +-0.0038\n",
      "alpha=0.001, power_t=10 \t accuracy=0.7510 +-0.0036\n",
      "alpha=0.001, power_t=100 \t accuracy=0.7509 +-0.0040\n",
      "alpha=0.001, power_t=500 \t accuracy=0.7509 +-0.0037\n",
      "alpha=0.001, power_t=1000 \t accuracy=0.7511 +-0.0041\n",
      "alpha=0.01, power_t=0.1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=0.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=1.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=2 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=3 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=10 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=100 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=500 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=1000 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=0.1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=0.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=1.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=2 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=3 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=10 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=100 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=500 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=1000 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=0.1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=0.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=1.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=2 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=3 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=10 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=100 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=500 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=1000 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=0.1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=0.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=1.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=2 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=3 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=10 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=100 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=500 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=1000 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=0.1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=0.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=1.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=2 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=3 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=10 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=100 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=500 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=1000 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=0.1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=0.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=1.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=2 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=3 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=10 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=100 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=500 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=1000 \t accuracy=0.6104 +-0.0001\n",
      "alpha=10, power_t=0.1 \t accuracy=0.5510 +-0.1188\n",
      "alpha=10, power_t=0.5 \t accuracy=0.6206 +-0.0202\n",
      "alpha=10, power_t=1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=10, power_t=1.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=10, power_t=2 \t accuracy=0.6104 +-0.0001\n",
      "alpha=10, power_t=3 \t accuracy=0.6104 +-0.0001\n",
      "alpha=10, power_t=5 \t accuracy=0.6216 +-0.0137\n",
      "alpha=10, power_t=10 \t accuracy=0.6104 +-0.0001\n",
      "alpha=10, power_t=100 \t accuracy=0.6104 +-0.0001\n",
      "alpha=10, power_t=500 \t accuracy=0.6104 +-0.0001\n",
      "alpha=10, power_t=1000 \t accuracy=0.6104 +-0.0001\n",
      "CPU times: user 4min 9s, sys: 36.7 ms, total: 4min 9s\n",
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_alpha = -1\n",
    "best_t = -1\n",
    "for cur_alpha, cur_t in itertools.product([0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 2, 4, 5, 10],\n",
    "                                          [0.1, 0.5, 1, 1.5, 2, 3, 5, 10, 100, 500, 1000]):\n",
    "    cls = SGDClassifier(alpha=cur_alpha, power_t=cur_t)\n",
    "    cv_scores = cross_val_score(cls, X, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_alpha = cur_alpha\n",
    "        best_t = cur_t\n",
    "    print('alpha={0}, power_t={1}'.format(cur_alpha, cur_t), '\\t', \n",
    "          'accuracy={0:0.4f}'.format(cur_avg_acc), \n",
    "          '+-{0:0.4f}'.format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are: alpha=0.0001, power_t=1.5 with accuracy=0.7875\n"
     ]
    }
   ],
   "source": [
    "print('Best params are: alpha={0}, power_t={1} with accuracy={2:0.4f}'.format(best_alpha, best_t, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PassiveAgressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01 \t accuracy=0.7700 +-0.0031\n",
      "C=0.1 \t accuracy=0.7895 +-0.0022\n",
      "C=0.5 \t accuracy=0.7674 +-0.0045\n",
      "C=1 \t accuracy=0.7556 +-0.0023\n",
      "C=5 \t accuracy=0.7489 +-0.0046\n",
      "C=10 \t accuracy=0.7454 +-0.0048\n",
      "C=100 \t accuracy=0.7472 +-0.0056\n",
      "C=200 \t accuracy=0.7441 +-0.0057\n",
      "C=500 \t accuracy=0.7500 +-0.0024\n",
      "C=1000 \t accuracy=0.7472 +-0.0033\n",
      "C=10000 \t accuracy=0.7444 +-0.0053\n",
      "C=15000 \t accuracy=0.7421 +-0.0043\n",
      "C=20000 \t accuracy=0.7454 +-0.0069\n",
      "C=100000 \t accuracy=0.7451 +-0.0066\n",
      "CPU times: user 31 s, sys: 0 ns, total: 31 s\n",
      "Wall time: 31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_C = -1\n",
    "for cur_C in [0.01, 0.1, 0.5, 1, 5, 10, 100, 200, 500, 1000, 10000, 15000, 20000, 100000]:\n",
    "    cls = PassiveAggressiveClassifier(C=cur_C)\n",
    "    cv_scores = cross_val_score(cls, X, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_C = cur_C\n",
    "    print('C={0}'.format(cur_C), '\\t', \n",
    "          'accuracy={0:0.4f}'.format(cur_avg_acc), \n",
    "          '+-{0:0.4f}'.format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are: C=0.1 with accuracy=0.7895\n"
     ]
    }
   ],
   "source": [
    "print('Best params are: C={0} with accuracy={1:0.4f}'.format(best_C, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.01 \t accuracy=0.7129 +-0.0064\n",
      "alpha=0.05 \t accuracy=0.7332 +-0.0061\n",
      "alpha=0.1 \t accuracy=0.7473 +-0.0040\n",
      "alpha=0.5 \t accuracy=0.7783 +-0.0016\n",
      "alpha=1 \t accuracy=0.7840 +-0.0017\n",
      "alpha=1.5 \t accuracy=0.7864 +-0.0029\n",
      "alpha=3 \t accuracy=0.7859 +-0.0026\n",
      "alpha=5 \t accuracy=0.7848 +-0.0025\n",
      "alpha=10 \t accuracy=0.7808 +-0.0027\n",
      "alpha=100 \t accuracy=0.7147 +-0.0034\n",
      "alpha=250 \t accuracy=0.6306 +-0.0011\n",
      "alpha=500 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1000 \t accuracy=0.6104 +-0.0001\n",
      "CPU times: user 6min 22s, sys: 983 ms, total: 6min 23s\n",
      "Wall time: 6min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_alpha = -1\n",
    "for cur_alpha in [0.01,0.05, 0.1, 0.5, 1, 1.5, 3, 5, 10, 100, 250, 500, 1000]:\n",
    "    cls = RidgeClassifier(alpha=cur_alpha)\n",
    "    cv_scores = cross_val_score(cls, X, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_alpha = cur_alpha\n",
    "    print('alpha={0}'.format(cur_alpha), '\\t', \n",
    "          'accuracy={0:0.4f}'.format(cur_avg_acc), \n",
    "          '+-{0:0.4f}'.format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are: alpha=1.5 with accuracy=0.7844\n"
     ]
    }
   ],
   "source": [
    "print('Best params are: alpha={0} with accuracy={1:0.4f}'.format(best_alpha, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The winner among the linear models - PassiveAgressiveClassifier with accuracy=0.7895"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2). Bayesian Classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.8 s, sys: 143 ms, total: 9.94 s\n",
      "Wall time: 9.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_alpha = -1\n",
    "for cur_alpha in [0.01, 0.05, 0.1, 0.5, 1, 1.5, 3, 5, 10, 100, 250, 500]:\n",
    "    clf = BernoulliNB(alpha=cur_alpha)\n",
    "    cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_alpha = cur_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are: alpha=3 with accuracy=0.7262\n"
     ]
    }
   ],
   "source": [
    "print('Best params are: alpha={0} with accuracy={1:0.4f}'.format(best_alpha, best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.94 s, sys: 40 ms, total: 5.98 s\n",
      "Wall time: 5.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_alpha = -1\n",
    "grid = np.linspace(0.01, 0.1, 10)\n",
    "for cur_alpha in grid:  # [0.01, 0.05, 0.1, 0.5, 1, 1.5, 3, 5, 10, 100, 250, 500]:\n",
    "    clf = MultinomialNB(alpha=cur_alpha)\n",
    "    cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_alpha = cur_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are: alpha=0.05000000000000001 with accuracy=0.7658\n"
     ]
    }
   ],
   "source": [
    "print('Best params are: alpha={0} with accuracy={1:0.4f}'.format(best_alpha, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3). Metric Classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min, sys: 7.53 s, total: 3min 7s\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_k = -1\n",
    "for cur_k in [10, 25, 50, 100]:  # [1, 3, 5] have already given worse result\n",
    "    clf = KNeighborsClassifier(n_neighbors=cur_k)\n",
    "    cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_k = cur_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are: k=25 with accuracy=0.7550\n"
     ]
    }
   ],
   "source": [
    "print('Best params are: k={0} with accuracy={1:0.4f}'.format(best_k, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4). Ensembles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.749654463571\n",
      "CPU times: user 6min 16s, sys: 26.7 ms, total: 6min 16s\n",
      "Wall time: 6min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = AdaBoostClassifier()\n",
    "cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=5)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.746783227622\n",
      "CPU times: user 46min 12s, sys: 547 ms, total: 46min 13s\n",
      "Wall time: 46min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = BaggingClassifier()\n",
    "cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=5)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.737860450952\n",
      "CPU times: user 1min 4s, sys: 6.67 ms, total: 1min 4s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = ExtraTreesClassifier()\n",
    "cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=5)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.737446439707\n",
      "CPU times: user 39.8 s, sys: 3.33 ms, total: 39.8 s\n",
      "Wall time: 39.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = RandomForestClassifier()\n",
    "cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=5)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1). Gradient Boosting Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "# import lightgbm as lgb  ## can`t install :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-d22c237c4e12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clf = GradientBoostingClassifier()\\ncv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=3)\\nprint(cv_scores.mean())'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2101\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2103\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2104\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m                                               fit_params)\n\u001b[0;32m-> 1433\u001b[0;31m                       for train, test in cv)\n\u001b[0m\u001b[1;32m   1434\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateComputeBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m         \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m             \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1606\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m         raise ValueError(\"scoring must return a number, got %s (%s) instead.\"\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mScore\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mapplied\u001b[0m \u001b[0mto\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0mof\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0mon\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1496\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m         \"\"\"\n\u001b[0;32m-> 1498\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1499\u001b[0m         \u001b[0mdecisions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_score_to_decision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecisions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1454\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m         \"\"\"\n\u001b[0;32m-> 1456\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1457\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n\u001b[0;32m--> 371\u001b[0;31m                                       force_all_finite)\n\u001b[0m\u001b[1;32m    372\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \"\"\"\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[1;32m    239\u001b[0m                         \u001b[0;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                         'convert to a dense numpy array.')\n",
      "\u001b[0;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = GradientBoostingClassifier()\n",
    "cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=3)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, I have some problems with *sklearns GBClf* and *LightGBM clf*, later I'll fix it, but now let`s try **XGBoost**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.757746536539\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=3)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let`s try to make a **bigrams** and **trigrams** on **pre-cleaned with nltk** data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#nltk.download()  # uncomment if you use nltk for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кружк\n",
      "чашк\n",
      "бегущ\n",
      "дела\n"
     ]
    }
   ],
   "source": [
    "stemmer = RussianStemmer()\n",
    "print(stemmer.stem(\"Кружки\"))\n",
    "print(stemmer.stem(\"Чашки\"))\n",
    "print(stemmer.stem(\"бегущий\"))\n",
    "print(stemmer.stem(\"делающий\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кружки\n",
      "бегущий\n",
      "анклавы\n",
      "лучше\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"Кружки\"))\n",
    "print(lemmatizer.lemmatize(\"бегущий\"))\n",
    "print(lemmatizer.lemmatize(\"анклавы\"))\n",
    "print(lemmatizer.lemmatize(\"лучше\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we can see **stemming** and **lemmatization** for russian language in **nltk** is a bit **'strange'**.  \n",
    "Let`s try **pymorphy** (https://pymorphy2.readthedocs.io/en/latest/user/guide.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='стали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='стать', score=0.984662, methods_stack=((<DictionaryAnalyzer>, 'стали', 904, 4),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 1),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 2),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 5),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 6),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 9),))]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "morph.parse('стали')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бежать кружка анклав молодец хороший\n"
     ]
    }
   ],
   "source": [
    "print(morph.parse('бегущий')[0].normal_form,\n",
    "      morph.parse('кружки')[0].normal_form,\n",
    "      morph.parse('Анклавы')[0].normal_form,\n",
    "      morph.parse('молодец')[0].normal_form,\n",
    "      morph.parse('лучше')[0].normal_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that pymorphy handles with russian very good, so now I am going to:\n",
    "- apply **.parse().normal_form** to corpus of sentiments;\n",
    "- make **tf-idf** for this corpus with **bigrams and trigrams**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with pymorpy2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я имею кредитную карту. Пользуюсь ею длительное время. Ни разу не пожалела об этом! Возникающие вопросы направляла в банк и в течение одного дня получала квалифицированные ответы. Отзывчивость, доброжелательность и индивидуальный подход - особенность этого банка. Ни один мой вопрос не остался без ответа. В рамках этого отзыва не могу описать возникшие у меня вопросы. Достаточно сказать, что надежды на их решение у меня не было даже в принципе. Тем не менее, сотрудники банка решили все мои проблемы. Тинькофф банк - это современный прогрессивный банк, ценящий своих клиентов, имеет огромный потенциал в виде высокопрофессиональных сотрудников. Буду рекомендовать друзьям и знакомым!\n"
     ]
    }
   ],
   "source": [
    "X = data['sentiment']\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Tokenizing with nltk**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет!', 'Меня зовут Илья, а тебя?', ':)']\n",
      "['Привет', '!', 'Меня', 'зовут', 'Илья', ',', 'а', 'тебя', '?', ':', ')']\n"
     ]
    }
   ],
   "source": [
    "text = \"Привет! Меня зовут Илья, а тебя? :)\"\n",
    "\n",
    "print(sent_tokenize(text))\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-cba92d0e36fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Привет! Меня зовут Илья, а тебя? :)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Тут я хочу'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'протестировать токенизацию'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'на несколких предлжениях.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[1;32m    110\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[1;32m     93\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m    309\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "text = ['Привет! Меня зовут Илья, а тебя? :)', 'Тут я хочу', 'протестировать токенизацию', 'на несколких предлжениях.']\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*word_tokenize()* doesn`t work with lists of strings, so I must loop over all the corpus and than loop over all the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['привет', '!', 'я', 'звать', 'илья', ',', 'а', 'ты', '?', ':', ')']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[morph.parse(token)[0].normal_form for token in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pymorphy_lemmatize(sentiment):\n",
    "    '''Returns new, lemmatized sentiment'''\n",
    "    \n",
    "    tokenized_sentiment = word_tokenize(sentiment)\n",
    "    new_sentiment = ''\n",
    "    return ' '.join([morph.parse(token)[0].normal_form for token in tokenized_sentiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'привет ! это тест функция , который токенизировать и лемматизировать отзыв . : )'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pymorphy_lemmatize('Привет! Это тест функции, которая токенизирует и лемматизирует отзывы. :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pymorphy_transform(corpus):\n",
    "    '''Returns corpus of lemmatized sentiments'''\n",
    "    \n",
    "    return [pymorphy_lemmatize(sentiment) for sentiment in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'Я имею кредитную карту. Пользуюсь ею длительное время. Ни разу не пожалела об этом! Возникающие вопросы направляла в банк и в течение одного дня получала квалифицированные ответы. Отзывчивость, доброжелательность и индивидуальный подход - особенность этого банка. Ни один мой вопрос не остался без ответа. В рамках этого отзыва не могу описать возникшие у меня вопросы. Достаточно сказать, что надежды на их решение у меня не было даже в принципе. Тем не менее, сотрудники банка решили все мои проблемы. Тинькофф банк - это современный прогрессивный банк, ценящий своих клиентов, имеет огромный потенциал в виде высокопрофессиональных сотрудников. Буду рекомендовать друзьям и знакомым!'\n",
      " 'Всем привет! Я в этом банке , как только рухнул СВЯЗНОЙ БАНК , очень доволен обслуживанием и функционалом, если и возникали проблемы, то были решаемы очень быстро по телефону в общении со специалистами банка, всеми консультациями доволен, работу ребята делают на твердую ПЯТЕРКУ!!! Вот и сегодня меня по моей собственной ошибке обманули, я сразу в тех поддержку, карту заблокировали, проконсультировали, тут же оформили новую карту, при чем \\xa0, сотрудник банка Алексей ( вн. номер 61145) очень быстро решил мою проблему, \\xa0терпеливо, вежливо и доходчиво объяснил мне в чем я совершил ошибку и организовал мне близлежащую встречу с сотрудником банка (курьером) \\xa0, который мне и передаст мою новую дебетовую карту. Многие мои друзья так же , как и я пользуются услугами Тинькофф банка по моей рекомендации и очень довольны: проценты, кешбек, обслуживание. Так же радует акция \" приведи друга\" на днях снова получил свои 750 рублей.\\xa0p.s\\xa0Если руководство банка читает мой отзыв, прошу вас поощрить вашего сотрудника Алексея (вн. номер 61145)'\n",
      " 'Добрый вечер.Был вашим вкладчиком на протяжении 2015-2016г,\\xa0 мне предложили \\xa0в\\xa0 вашембанке отделение Красная Пресня воспользоваться кредитной картой, я получил\\xa0 золотую карту MASTERCARD. На протяжении года я ей пользовалсяуспешно, до недавнего времени.1.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0При открытии карты меня не кто не предупредил,что мне подключили услугу ЗАЩИТА КАРТЫ, (Я НЕ ПОДПИСЫВАЛ ДОКУМЕНТЫ НАПОДКЛЮЧЕНИЕ ДАННОЙ УСЛУГИ) за эту услугу ВАШ БАНК\\xa0 списывал с моей карты денежные средства. \\xa0Жалобу на необоснованное списания я сегодня 16ноября 2016 года в отделение Красная Пресня я оформил №1-1587039. ПРОШУ ВЕРНУТЬДЕНЬГИ В ПОЛНОМ ОБЪЕМЕ2.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Яделал перевод с карты БИНБАНКА\\xa0 на другуюкарту ТИНЬКОФ как покупку\\xa0 на протяжениивсего пользования, никогда комиссий не было, но недавно, а именно 22 октября2016г оформил покупку на банк ТИНЬКОФ 40000 рублей и у меня списали комиссию вразмере 1554,11 руб, я пришел в отделение на Красной Пресне и оформилпретензию\\xa0 за № 1-10674636573 от26.10.2016 ответ так и не поступил. Прочее комиссии списанные с меня по карте\\xa0 в ноябре в размере 1,500 руб также никто неможет объяснить(ЗА ЧТО СПИСАЛИ С МЕНЯ ЭТИ ДЕНЕЖНЫЕ СРЕДСТВА)\\xa0 ПРОШУ ВЕРНУТЬ ДЕНЬГИ В ПОЛНОМ ОБЪЕМЕ3.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Неоднократно звонил в ВАШ КОЛЛ-Центр,адекватного или удовлетворительно ответа от ваших сотрудников\\xa0 разъяснить мне\\xa0\\xa0 на каком основании с меня были списаныденежные средства я\\xa0\\xa0 не получил!!! \\xa016ноября 2016г я закрыл вашу кредитную карту,в связи с утратой моего терпения и\\xa0доверия как клиента, как вкладчика к Вашему Банку. В случае непринятиямер оставляю за собой право обратиться в контролирующие органы Банка России.4.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Прошел месяц с моей первой жалобы ответа от вашегобанка\\xa0 я так и не получил!!!'\n",
      " 'Очень разочарована банком ВТБ24. За смс уведомление деньги банк снимает вовремя, а сами уведомления,видимо,пешком идут. И виноват не банк,как сообщил сотрудник, а оператор связи! Видимо, совсем плохи дела у банка, т.к деньги, поступившие на счет кредитной карты, внимание!!! должны сутки побыть на карте, в противном случае их система не засчитывает! Это как надо неуважительно относиться к клиенту!'\n",
      " 'Отвратительный банк, и обслуживание. 24.11.2016 зарегистрировал киви Кошелек и его полностью идентифицировал, так на следующий день мне его заблокировали с суммой 19к рублей. отправил 2 запроса на разблокировку киви кошелька, так СБ и тех поддержка просто морозиться на мои сообщения отправлены мною им на почту. только пришло авто сообщение что моя заявка зарегистрирована и все.Прошу разобраться с этим вопросом номер тикета\\xa0№ 6626433.через форму на сайте были отправлены все потребываемые документыchernovalov.shura@mail.ruchernovalov.shura@mail.ru']\n"
     ]
    }
   ],
   "source": [
    "test = X[0:5].values\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pymorphy_transform(test))\n",
    "print(len(pymorphy_transform(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!! ATTENTION !!!**: next cell executes more than 20 minutes, be careful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_lemmatized = pymorphy_transform(X.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the new, lemmatized data to the .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = './data/lemmatized_responses_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatized_data = pd.DataFrame({'mark': data['mark'], 'sentiment': X_lemmatized})\n",
    "lemmatized_data.to_csv(file_name, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading new data** (made for testing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>mark</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>я иметь кредитный карта . пользоваться она дли...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>весь привет ! я в это банка , как только рухну...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>добрый вечер.быть ваш вкладчик на протяжение 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>очень разочаровать банк втб24 . за смс уведомл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>отвратительный банк , и обслуживание . 24.11.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  mark                                          sentiment\n",
       "0           0     5  я иметь кредитный карта . пользоваться она дли...\n",
       "1           1     5  весь привет ! я в это банка , как только рухну...\n",
       "2           2     1  добрый вечер.быть ваш вкладчик на протяжение 2...\n",
       "3           3     1  очень разочаровать банк втб24 . за смс уведомл...\n",
       "4           4     1  отвратительный банк , и обслуживание . 24.11.2..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_data = pd.read_csv(file_name, sep='\\t')\n",
    "lemmatized_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28916"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemmatized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let`s drop odd column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'mark', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatized_data.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mark</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>я иметь кредитный карта . пользоваться она дли...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>весь привет ! я в это банка , как только рухну...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>добрый вечер.быть ваш вкладчик на протяжение 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>очень разочаровать банк втб24 . за смс уведомл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>отвратительный банк , и обслуживание . 24.11.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mark                                          sentiment\n",
       "0     5  я иметь кредитный карта . пользоваться она дли...\n",
       "1     5  весь привет ! я в это банка , как только рухну...\n",
       "2     1  добрый вечер.быть ваш вкладчик на протяжение 2...\n",
       "3     1  очень разочаровать банк втб24 . за смс уведомл...\n",
       "4     1  отвратительный банк , и обслуживание . 24.11.2..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatized_data.to_csv(file_name, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Now we have dataset of lemmatized sentiments, let`s train Tf*Idf with bigrams and then re-train our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28916, 1357308)\n",
      "CPU times: user 17.8 s, sys: 147 ms, total: 18 s\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 2))\n",
    "\n",
    "X_lemmatized = lemmatized_data['sentiment']\n",
    "X_transformed = vectorizer.fit_transform(X_lemmatized)\n",
    "print(X_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = lemmatized_data['mark']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PassiveAgressiveClassifier (previous result - accuracy = 0.7895)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01 \t accuracy=0.7368 +-0.0040\n",
      "C=0.1 \t accuracy=0.7889 +-0.0017\n",
      "C=0.5 \t accuracy=0.7891 +-0.0012\n",
      "C=1 \t accuracy=0.7887 +-0.0014\n",
      "C=5 \t accuracy=0.7888 +-0.0017\n",
      "C=10 \t accuracy=0.7890 +-0.0019\n",
      "C=100 \t accuracy=0.7892 +-0.0017\n",
      "C=200 \t accuracy=0.7893 +-0.0019\n",
      "C=500 \t accuracy=0.7890 +-0.0011\n",
      "C=1000 \t accuracy=0.7890 +-0.0019\n",
      "C=10000 \t accuracy=0.7889 +-0.0017\n",
      "C=15000 \t accuracy=0.7889 +-0.0016\n",
      "C=20000 \t accuracy=0.7893 +-0.0013\n",
      "C=100000 \t accuracy=0.7891 +-0.0014\n",
      "CPU times: user 1min 56s, sys: 5.91 s, total: 2min 1s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_C = -1\n",
    "for cur_C in [0.01, 0.1, 0.5, 1, 5, 10, 100, 200, 500, 1000, 10000, 15000, 20000, 100000]:\n",
    "    cls = PassiveAggressiveClassifier(C=cur_C)\n",
    "    cv_scores = cross_val_score(cls, X_transformed, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_C = cur_C\n",
    "    print('C={0}'.format(cur_C), '\\t', \n",
    "          'accuracy={0:0.4f}'.format(cur_avg_acc), \n",
    "          '+-{0:0.4f}'.format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are: C=20000 with accuracy=0.7893\n"
     ]
    }
   ],
   "source": [
    "print('Best params are: C={0} with accuracy={1:0.4f}'.format(best_C, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, I have worse result, than before lemmatization, let`s try SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_C = -1\n",
    "for cur_C in [0.01]:#, 0.1, 0.5, 1, 5, 10, 100, 200, 500, 1000, 10000]:\n",
    "    cls = SVC(C=cur_C)\n",
    "    cv_scores = cross_val_score(cls, X_transformed, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_C = cur_C\n",
    "    print('C={0}'.format(cur_C), '\\t', \n",
    "          'accuracy={0:0.4f}'.format(cur_avg_acc), \n",
    "          '+-{0:0.4f}'.format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Best params are: C={0} with accuracy={1:0.4f}'.format(best_C, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.764040807058\n",
      "CPU times: user 48min 1s, sys: 8.18 s, total: 48min 9s\n",
      "Wall time: 10min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = xgb.XGBClassifier()\n",
    "cv_scores = cross_val_score(clf, X_transformed, y, scoring=\"accuracy\", cv=3)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous result was 0.7577, so we have improved the model, parameter tuning can make > 0.8 accuracy, but for now that`s all. \n",
    "\n",
    "I am going to save best models and vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(PassiveAggressiveClassifier(C=0.1), \"./classifiers/BestClassifier.pkl\")\n",
    "joblib.dump(TfidfVectorizer(sublinear_tf=True), \"./vectorizers/BestVectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: \n",
    "- do the **oversampling**\n",
    "- parameter **tuning** for ensembles\n",
    "- give it a try to **reduce space dimension (PCA?)** and try **Lasso feature_selection** (http://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "- make the **GridSearch** for classifiers (might require an additional resources, such as Amazon machine, to end in an adequate time)\n",
    "- try **LDA** for feature transformation\n",
    "- **gensim Word2Vec**\n",
    "- use **other metrics** to measure classifiaction quality\n",
    "- Same task, but stated as a **Regression**\n",
    "- do some **visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
