{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Respalizer (c). Data analysis (classification).\n",
    "## Made by Ilya Zakharkin (github.com/izaharkin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural language processing (sentiment analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data: Given data - responses of people on different bank companies (two features - sentiment (text, in russian) and mark (integer, 1 <= mark <= 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: We must predict which mark will this person give consider his response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful materials:\n",
    "- scikit-learn example: http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py\n",
    "- article for beginners: https://habrahabr.ru/company/mlclass/blog/270591/\n",
    "- TJ texts classification: https://habrahabr.ru/post/327072/\n",
    "- MIPT NLP course: https://github.com/canorbal/NLP_MIPT/\n",
    "- Russian language processing research: http://corpus.leeds.ac.uk/mocky/\n",
    "- example with nltk: http://streamhacker.com/2012/11/22/text-classification-sentiment-analysis-nltk-scikitlearn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "import joblib\n",
    "import _pickle\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let`s have a look at the data - how much data do we have, and in what format is it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/responses_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28916"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mark</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Я имею кредитную карту. Пользуюсь ею длительно...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Всем привет! Я в этом банке , как только рухну...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Добрый вечер.Был вашим вкладчиком на протяжени...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Очень разочарована банком ВТБ24. За смс уведом...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Отвратительный банк, и обслуживание. 24.11.201...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mark                                        description\n",
       "0     5  Я имею кредитную карту. Пользуюсь ею длительно...\n",
       "1     5  Всем привет! Я в этом банке , как только рухну...\n",
       "2     1  Добрый вечер.Был вашим вкладчиком на протяжени...\n",
       "3     1  Очень разочарована банком ВТБ24. За смс уведом...\n",
       "4     1  Отвратительный банк, и обслуживание. 24.11.201..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let`s change 'description' to 'sentiment':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_columns = data.columns.values\n",
    "new_columns[1] = 'sentiment'\n",
    "data.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28916.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.081685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.591598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mark\n",
       "count  28916.000000\n",
       "mean       2.081685\n",
       "std        1.591598\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        1.000000\n",
       "75%        3.000000\n",
       "max        5.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 29k responses, no blank fields, so it is data without holes in it, sentiments are in russian language, marks are from 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f6b15daafd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAFWCAYAAAB93nQWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XtclHX+9/H3MIgnDiLCkIaWRa3r2UAiEH9ieMIDqdxa\nm7taLrdpmlluaq0HNLOTmbqVrGa1669SQ9xk21RWxUNZtj9TK+9dTQwPDIQkeGIUrvuP7uaONCVg\nxC++no+Hjwfznev6zuf7AXnPdV3DjM2yLEsAAMBYXrVdAAAAqB7CHAAAwxHmAAAYjjAHAMBwhDkA\nAIYjzAEAMBxhDvwClmVp6tSpioyM1NChQ6/a495+++06fPjwVXu8y9m5c6fi4uIqte3f/vY3PfDA\nAzX22CNGjNCqVatqbL7aNmXKFL300ks1Mldd6w1+GcIcNWrEiBGKjIyUy+Wq7VIuKT09Xffee2+V\n9//ss8+0fft2bdmyRatXr67ByuqmgQMH6vXXX6/SvosWLdLjjz9ewxVBkuLj47Vjx47aLgM1iDBH\njTly5Ih27dolm82mrKys2i7HI44ePaoWLVqoUaNGtV1Krbhw4UJtlwDgEghz1JiMjAx17NhR99xz\njzIyMircN2XKFM2cOVOjR49W586dNXz4cBUUFOjpp59WZGSk+vTpoy+//NK9/cGDBzVixAhFREQo\nMTGxwpODn55O/OnR9u233663335bvXr1UkREhGbNmiXLsnTw4EHNmDFDu3fvVufOnRUREXHJdTid\nTo0ZM0Zdu3ZVQkKCVq5cKUlatWqVnnrqKff+CxcuvOT+q1evVt++fRUZGakHH3xQR48edd83Z84c\nde/eXV26dNHgwYO1a9cu931lZWV67bXXdPfdd6tz584aPHiwjh8/7r5/x44dF63pUhYtWqQJEybo\n8ccfV+fOnTVgwAAdOnRIS5YsUXR0tLp3765t27a5t3/vvffUt29fde7cWT179tQ777zjvu+HU+pp\naWmKiYnR1KlTL3q8t956S/369VNeXt5F91X2e/NT2dnZWrJkiT744AN17txZAwcOdN939OhRDR8+\nXJ07d9YDDzygEydOuO/bvXu3hg8froiICA0cOFA7d+68ZI8kKS0tTd26dVPnzp3Vu3dvffTRR5Kk\nPXv2aNiwYYqIiFBsbKxSU1MrnGm6/fbbtWLFCvXq1UudO3fWggUL9M0332j48OHq0qWLHnnkEff2\nP/TvtddeU1RUlOLj4/W3v/3tZ2vatGmTBg0apIiICA0fPlz79+//2W23b9+uPn366I477lBqamqF\nPn7zzTf67W9/q6ioKEVFRemxxx5TcXGxJGny5Mk6duyYxowZo86dO+vPf/6zJGnChAmKiYnRHXfc\nod/85jf6z3/+87OPjWuQBdSQu+++2/rrX/9q7d271/r1r39tFRQUuO974oknrK5du1p79+61zp07\nZ40YMcLq0aOHtWbNGuvChQvW/Pnzrfvvv9+yLMtyuVzW3Xffbb366qtWaWmptWPHDqtTp07WwYMH\nLcuyrPvvv99auXKle+733nvPGj58uPv2bbfdZqWkpFgnT560jh49akVFRVlbtmy55LaXct9991kz\nZsywzp07Z3355ZdWVFSUtWPHjkrtv2HDBuvuu++2Dhw4YJ0/f97605/+ZA0bNsx9f0ZGhnXixAnr\n/Pnz1rJly6y77rrLOnfunGVZlvXnP//Z6t+/v3Xw4EGrvLzc+uqrr6wTJ05ccU0/tXDhQqtdu3ZW\ndna2df78eWvy5MlWjx49rFdeecVyuVzWu+++a/Xo0cO9/aZNm6zDhw9b5eXl1s6dO60OHTpY+/bt\nsyzLsj7++GOrTZs21nPPPWeVlpZaZ8+etT7++GOrW7dulmVZ1qJFi6ykpCSrsLDwkrX8ku/Npdbx\n2GOPVRi7//77rZ49e1pff/21dfbsWev++++3nn/+ecuyLCsvL8/q2rWrtXnzZqusrMzatm2b1bVr\n10vWdvDgQSsuLs7Ky8uzLMuycnNzrcOHD1uWZVl79+61/ud//sc6f/68lZuba/Xp08davnx5hTWM\nGTPGKikpsf79739bbdu2tX77299a33zzjVVcXGz17dvXSk9Pr9C/uXPnWqWlpdbOnTutjh07un+W\nn3jiCWv+/PmWZVnWF198Yd15553W7t27rQsXLljp6elWjx49rNLS0ovqLywstDp16mR98MEHlsvl\nspYvX261adPG/f8iJyfH2rZtm1VaWmoVFhZa9913nzVnzhz3/j169LC2b99eYc5Vq1ZZJSUlVmlp\nqTVnzhxr4MCBl/y+4NrEkTlqxK5du3Ts2DH17dtX7dq1U1hYmNatW1dhm4SEBLVr107169dXQkKC\n6tevr6SkJNntdvXr109fffWVJOnzzz/XmTNnlJKSIh8fH0VHR6tHjx7KzMysdD2///3v5e/vr+bN\nmysqKuqyRzg/dvz4cf3rX//S448/rvr166tNmzZKTk7W2rVrK7X/O++8o5SUFN1yyy3y9vbWmDFj\n9NVXX7mPzgcNGqTAwEB5e3vrgQcekMvl0qFDhyR9f+T/yCOPqHXr1rLZbPrVr36lwMDAKq0pIiJC\n3bp1k7e3t/r06aOioiKlpKSoXr166tevn44ePeo+Uvuv//ovtWzZUjabTV27dlVMTEyFMwZeXl6a\nMGGCfHx81KBBA0nfvxDwmWee0fbt2/XWW2+padOmlerPL13HpQwePFg333yzGjRooD59+rh/btau\nXau4uDh1795dXl5eiomJUbt27bRly5aL5rDb7XK5XDp48KDOnz+vG2+8US1btpQktWvXTp06dZK3\nt7duvPFGDRs2TJ9++mmF/UePHi1fX1+Fh4frtttuU0xMjMLCwuTn56e4uLgKZ5kk6ZFHHpGPj4+6\ndu2q7t2764MPPriopnfffVfDhg1Tx44dZbfbdc8996hevXravXv3RdtmZ2crPDxcffr0Ub169fS7\n3/1OzZo1c9/fqlUrxcTEyMfHR02bNtWoUaMuWsNPDR06VL6+vvLx8dH48eO1f/9+lZSUXHYfXDu8\na7sA1A0ZGRmKiYlx/1Lv37+/1qxZo5EjR7q3CQoKcn/doEGDCr98GjRooDNnzkiS8vPzFRoaKi+v\n//9cs3nz5nI6nZWuJzg42P11w4YNdfr06Urtl5+fr4CAAPn6+lZ47H379lVq/2PHjmnu3Ll69tln\n3WOWZcnpdKpFixZatmyZVq9erfz8fNlsNp06dUpFRUWSpLy8PHegVHdNP+11YGCg7Ha7+7YknTlz\nRv7+/tqyZYv+9Kc/KScnR+Xl5Tp37pxuu+029/6BgYGqX79+hflLSkq0cuVKvfTSS/Lz86tMa6q0\njsrs/8PPzbFjx/SPf/xDmzZtct9/4cIFRUVFXTRHq1atNG3aNC1atEgHDhxQbGyspkyZIofDoUOH\nDmnevHnat2+fzp49q7KyMrVt27bC/j/+2a1fv/5Ft7/99lv3bX9//wqvsWjevLny8/MvqunYsWPK\nyMjQX//6V/fY+fPnL7ntD/9HfmCz2XTDDTe4b3/77bd6+umntWvXLp0+fVqWZcnf3/+ieX5QVlam\nl156Sf/4xz904sQJ9/+9oqKiX/z9Re0gzFFt586d0wcffKDy8nLFxMRIklwul4qLi7V//3796le/\n+kXzhYSEKC8vT+Xl5e5fKsePH9dNN90k6ftf4GfPnnVv/+NfnFdis9mu+NgnT57UqVOn3IF+/Phx\nORyOSs1/ww03aMyYMRWu8f5g165dWrp0qd544w2Fh4fLy8tLkZGR7mudoaGh+uabbyoEqae5XC5N\nmDBBzz77rHr27Kl69epp7NixFa6/Xqpn/v7+ev755zVx4kQtXrxYd9xxR43XdqXv1U/dcMMNGjRo\nkObMmVOp7QcMGKABAwbo1KlTmj59ul544QU9//zzmjlzpn7961/rxRdflK+vr9544w19+OGHVVmC\nJKm4uFhnzpxxB/rx48cVHh5+yfrHjBmjhx566IpzBgcHV3iNgmVZFV5fMX/+fNlsNr3//vtq0qSJ\nNm7cqNTU1J+d7/3331dWVpaWL1+uG2+8USUlJRV+NnHt4zQ7qm3jxo2y2+3KzMxURkaGMjIy9Pe/\n/10REREXvRCuMjp06KAGDRpo6dKlOn/+vHbu3Kl//vOf6tevnySpTZs22rBhg86ePavDhw//oj8R\nCwoKktPp/Nk/nbvhhhvUuXNnzZ8/X6Wlpdq/f79Wr159yXC+lOHDhystLc394qGSkhL3KdXTp0/L\nbreradOmunDhghYvXqxTp065901OTtbLL7+snJwcWZal/fv3u4/aPcXlcsnlcqlp06by9vbWli1b\ntH379krtGxUVpRdeeEHjx4/Xnj17ary2oKAgHT16VOXl5ZXafuDAgdq0aZO2bt2qsrIylZaWaufO\nnZd8Yd7XX3+tjz76SC6XSz4+Pqpfv777iePp06fVuHFjNW7cWAcPHtTbb79d7bUsWrRILpdLu3bt\n0ubNm9WnT5+LtklOTtY777yjzz//XJZl6cyZM9q8eXOFn5EfdO/eXf/5z3+0fv16XbhwQW+99VaF\nJ7WnT59Wo0aN5OfnJ6fTqaVLl1bYv1mzZsrNza2wvY+PjwIDA3X27FnNnz+/2mvG1UWYo9rWrFmj\nwYMHq3nz5goODnb/+81vfqP333//F/85k4+Pj1577TVlZ2frzjvv1KxZs/Tcc8/plltukST97ne/\nU7169XTXXXfpiSee0IABAyo995133qlbb71VsbGxlzz9Kn1/VHP06FF169ZNDz/8sMaPH6+77rqr\nUvMnJCRo9OjRmjRpkrp06aL+/fsrOztbkhQbG6tu3bqpd+/eio+PV/369SucGh01apT69u2rBx54\nQF26dNGTTz6p0tLSSq+tKnx9ffXUU09p4sSJioyM1Lp16xQfH1/p/WNiYjR37lyNGTNGX3zxRY3W\n9kPgRUVF6Z577rni9jfccINeeeWVCq/aX7Zs2SWfDLhcLr344ouKiopSbGysTpw4oUmTJkmSnnji\nCa1bt05dunTRH//4R/eTyKpq1qyZ/P391a1bNz3++OOaOXOm+2f5x9q3b6/Zs2crNTVVkZGR6tWr\nl9LT0y85Z9OmTfXyyy+713D48GF16dLFff/DDz+sL7/8UhEREUpJSVGvXr0q7J+SkqJXX31VERER\nWrZsmZKSktS8eXN169ZNiYmJ6tSpU7XWjKvPZnEeBQA8YufOnZo8ebL7CR3gKRyZAwBgOMIcAADD\ncZodAADDcWQOAIDhjP0784ICs96ZKDCwkYqKztR2GXUeffY8eux59PjqMK3PwcE//wY+HJlfJd7e\n9tou4bpAnz2PHnsePb466lKfCXMAAAxHmAMAYDjCHAAAwxHmAAAYjjAHAMBwhDkAAIYjzAEAMBxh\nDgCA4QhzAAAMR5gDAGA4whwAAMMR5gAAGO6Kn5o2depUbd68WUFBQVq3bp0kaeLEiTp06JAkqaSk\nRH5+flq7dq2OHDmifv366eabb5YkdezYUampqZKkffv2aerUqTp37py6d++uJ598UjabTd99950e\nffRRHT16VC1atNCCBQsUEBDgqfUCAFDnXDHMBw8erPvvv19PPPGEe2zBggXur+fNmydfX1/37ZYt\nW2rt2rUXzTNz5kzNnj1bHTt21O9//3tlZ2ere/fuSktLU3R0tFJSUpSWlqa0tDRNnjy5uuuqsuAN\n/p6b20PzFiQUe2hmALiYJ39PXo4pv+uWLVui9PSVyszMumqPecXT7JGRkT97pGxZlj744AP179//\nsnPk5+fr1KlT6tSpk2w2m5KSkpSV9f0is7KylJSUJElKSkrSxo0bf+kaAAC4rl3xyPxydu3apaCg\nIN10003usSNHjigpKUm+vr6aOHGiIiIi5HQ6FRoa6t4mNDRUTqdTklRYWKiQkBBJUnBwsAoLCyv1\n2IGBjerUZ9FWx+U+sP56RD88jx57Hj2+mCd6UpNznj9/Xl5eXmrcuL5sNttV/R5WK8zXrVtX4ag8\nJCREmzZtUmBgoPbt26dx48YpMzOz0vPZbDbZbLZKbVtUdOYX11sZnjoV7kkFBSW1XcI1IzjYj354\nGD32vGu9x7X1e7IqPXn66Zn6+uuDevDB/61XXnlZx48fU5cuEfrjH1Pl7V2mJ56Yqq+++kKtWt2s\nqVOn69ZbwyVJb7/9V2VlrVdu7mH5+NRXmzZtNWHCJN14Y5h77ocfTlGTJk0UGXmnVqx4U3l5x7Vq\n1d90+nSpLMty12tZlhYseF4ffviBXnxxkdq2bVel9V/uyUGVw/zChQvasGGD0tPT3WM+Pj7y8fGR\nJLVr104tW7bUoUOH5HA4lJeX594uLy9PDodDkhQUFKT8/HyFhIQoPz9fTZs2rWpJAABcxOnM09Kl\nr+n3v39I586d00svPa/nnntaBQVO9e07UPfd91stWfInzZw5TX/5y0rZbDYVFDg1ZMj/ksMRqjNn\nTisj4z2NGfOA3nlnTYXXie3d+7mOHj2ihx4arwYNGlS4T5LKy8v1/PNztXXrZi1c+Kpuu+1XHllj\nlcN8x44dat26dYXT5ydOnFBAQIDsdrtyc3OVk5OjsLAwNWnSRL6+vtq9e7c6duyojIwMjRgxQpIU\nHx+vjIwMpaSkKCMjQz179qz+qgAA+H9KSoq1ZMlytWhxoyTp4MH/6L//+y969tlnFRPzQ+ZYmjx5\nog4fztFNN92sCRMec+9fVlamyMgo9e/fS1u3blbfvv1/NPcpLV/+32raNOiixy0rK9PcuTO1a9cn\nWrhwiVq3vsVja7ximE+aNEmffPKJioqKFBcXp/Hjxys5OVl///vflZiYWGHbTz/9VAsXLpS3t7e8\nvLw0a9YsNWnSRJI0Y8YM95+mxcXFKS4uTpKUkpKiiRMnavXq1WrevHmFV8oDAFBdoaE3uINcklq0\n+P5U+Z133nnRWEFBvm666Wbt27dXS5e+qn//+/+ouPike7vc3G8qzH377b/6mSAv14wZ0/Tll/u0\naFGaWrZsVaNr+qkrhvn8+fMvOT5v3ryLxnr37q3evXtfcvv27du7/079xwIDA/Xmm29eqQwAAKrE\n17fiteZ69epJkvz8/HTmTHmFMZfLpby8PE2a9LDatGmryZOnqlmzYNWrV0+TJ0+Uy+WqMNfPXRou\nLT2nnTt3qHv3eI8HuVTNF8ABAFDX7Ny5Q6Wl5zRv3otq2LChpO9fJ/bjI/Qf/NyLths1aqRZs57R\nH/4wUUFBzfTQQ+M9WjNv5woAwI+UlpbKZrPJbv//f/78z39uVFlZ2S+aJyKiq1JT5+mdd/6qN99c\nVtNlVsCROQAAP3LHHZEqLy/X3Lmz1L//IB069LXefvsvF52ur4zY2Dj98Y+pSk39oxo3bqyhQ4d7\noGLCHADwC5nytqpVdcstt2ratBl6/fU0ZWdv1q23hmv27Gc1Y8bUKs139929de7cWT333Fw1atRY\n/foNqOGKJZtlWVaNz3oVeOoNFWrrPYero67/x/olrvU326gL6LHn0eOrw7Q+X+5NY7hmDgCA4Qhz\nAAAMR5gDAGA4whwAAMMR5gAAGI4wBwDAcIQ5AACGI8wBADAcYQ4AgOEIcwAADEeYAwBgOMIcAADD\nEeYAABiOMAcAwHCEOQAAhiPMAQAwHGEOAIDhCHMAAAxHmAMAYDjCHAAAwxHmAAAYjjAHAMBwhDkA\nAIYjzAEAMBxhDgCA4QhzAAAMd8Uwnzp1qqKjo9W/f3/32KJFi9StWzcNGjRIgwYN0pYtW9z3LVmy\nRAkJCerdu7e2bt3qHs/Ozlbv3r2VkJCgtLQ093hubq6Sk5OVkJCgiRMnyuVy1dTaAAC4LlwxzAcP\nHqylS5deND5y5EitXbtWa9euVffu3SVJBw4cUGZmpjIzM7V06VLNmjVLZWVlKisrU2pqqpYuXarM\nzEytW7dOBw4ckCS98MILGjlypDZs2CB/f3+tXr26hpcIAEDddsUwj4yMVEBAQKUmy8rKUmJionx8\nfBQWFqZWrVppz5492rNnj1q1aqWwsDD5+PgoMTFRWVlZsixLH3/8sXr37i1Juueee5SVlVW9FQEA\ncJ3xruqOK1asUEZGhtq1a6cpU6YoICBATqdTHTt2dG/jcDjkdDolSaGhoRXG9+zZo6KiIvn7+8vb\n29u9zQ/bX0lgYCN5e9urWn6dEhzsV9slXFPoh+fRY8+jx1dHXelzlcL83nvv1dixY2Wz2fTyyy9r\n3rx5euaZZ2q6tssqKjrjkXmDPTKrZxUUlNR2CdeM4GA/+uFh9Njz6PHVYVqfL/fEo0qvZm/WrJns\ndru8vLyUnJysvXv3Svr+iDsvL8+9ndPplMPh+NnxwMBAFRcX68KFC5KkvLw8ORyOqpQEAMB1q0ph\nnp+f7/5648aNCg8PlyTFx8crMzNTLpdLubm5ysnJUYcOHdS+fXvl5OQoNzdXLpdLmZmZio+Pl81m\nU1RUlD788ENJ0po1axQfH18DywIA4PpxxdPskyZN0ieffKKioiLFxcVp/Pjx+uSTT7R//35JUosW\nLZSamipJCg8PV9++fdWvXz/Z7XZNnz5ddvv317WnT5+u0aNHq6ysTEOGDHE/AZg8ebIeffRRLViw\nQG3atFFycrKn1goAQJ1ksyzLqu0iqsJT1zmCN/h7ZF5PKkgoru0SrhmmXQMzET32PHp8dZjW5xq/\nZg4AAK4dhDkAAIYjzAEAMBxhDgCA4QhzAAAMR5gDAGA4whwAAMMR5gAAGI4wBwDAcIQ5AACGI8wB\nADAcYQ4AgOEIcwAADEeYAwBgOMIcAADDEeYAABiOMAcAwHCEOQAAhiPMAQAwHGEOAIDhCHMAAAxH\nmAMAYDjCHAAAwxHmAAAYjjAHAMBwhDkAAIYjzAEAMBxhDgCA4QhzAAAMR5gDAGA4whwAAMMR5gAA\nGO6KYT516lRFR0erf//+7rFnn31Wffr00YABAzRu3DgVFxdLko4cOaIOHTpo0KBBGjRokKZPn+7e\nZ9++fRowYIASEhI0Z84cWZYlSfruu+80atQo9erVS6NGjdLJkydreo0AANRpVwzzwYMHa+nSpRXG\nYmJitG7dOr3//vu66aabtGTJEvd9LVu21Nq1a7V27Vqlpqa6x2fOnKnZs2dr/fr1ysnJUXZ2tiQp\nLS1N0dHRWr9+vaKjo5WWllZTawMA4LpwxTCPjIxUQEBAhbHY2Fh5e3tLkjp16qS8vLzLzpGfn69T\np06pU6dOstlsSkpKUlZWliQpKytLSUlJkqSkpCRt3LixSgsBAOB65V3dCd577z317dvXffvIkSNK\nSkqSr6+vJk6cqIiICDmdToWGhrq3CQ0NldPplCQVFhYqJCREkhQcHKzCwsJKPW5gYCN5e9urW36d\nEBzsV9slXFPoh+fRY8+jx1dHXelztcL81Vdfld1u18CBAyVJISEh2rRpkwIDA7Vv3z6NGzdOmZmZ\nlZ7PZrPJZrNVatuiojNVqvlKgj0yq2cVFJTUdgnXjOBgP/rhYfTY8+jx1WFany/3xKPKYZ6enq7N\nmzfrjTfecAewj4+PfHx8JEnt2rVTy5YtdejQITkcjgqn4vPy8uRwOCRJQUFBys/PV0hIiPLz89W0\nadOqlgQAwHWpSn+alp2draVLl+rVV19Vw4YN3eMnTpxQWVmZJCk3N1c5OTkKCwtTSEiIfH19tXv3\nblmWpYyMDPXs2VOSFB8fr4yMDEmqMA4AACrnikfmkyZN0ieffKKioiLFxcVp/PjxSktLk8vl0qhR\noyRJHTt2VGpqqj799FMtXLhQ3t7e8vLy0qxZs9SkSRNJ0owZMzR16lSdO3dOcXFxiouLkySlpKRo\n4sSJWr16tZo3b64FCxZ4cLkAANQ9NuuHP/g2jKeucwRv8PfIvJ5UkFBc2yVcM0y7BmYieux59Pjq\nMK3Pl7tmzjvAAQBgOMIcAADDEeYAABiOMAcAwHCEOQAAhiPMAQAwHGEOAIDhCHMAAAxHmAMAYDjC\nHAAAwxHmAAAYjjAHAMBwhDkAAIYjzAEAMBxhDgCA4QhzAAAMR5gDAGA4whwAAMMR5gAAGI4wBwDA\ncIQ5AACGI8wBADAcYQ4AgOEIcwAADEeYAwBgOMIcAADDEeYAABiOMAcAwHCEOQAAhiPMAQAwHGEO\nAIDhKhXmU6dOVXR0tPr37+8e++677zRq1Cj16tVLo0aN0smTJyVJlmVpzpw5SkhI0IABA/TFF1+4\n91mzZo169eqlXr16ac2aNe7xffv2acCAAUpISNCcOXNkWVZNrQ8AgDqvUmE+ePBgLV26tMJYWlqa\noqOjtX79ekVHRystLU2SlJ2drZycHK1fv16zZ8/WzJkzJX0f/osXL9bKlSu1atUqLV682P0EYObM\nmZo9e7bWr1+vnJwcZWdn1+ASAQCo2yoV5pGRkQoICKgwlpWVpaSkJElSUlKSNm7cWGHcZrOpU6dO\nKi4uVn5+vrZt26aYmBg1adJEAQEBiomJ0datW5Wfn69Tp06pU6dOstlsSkpKUlZWVg0vEwCAusu7\nqjsWFhYqJCREkhQcHKzCwkJJktPpVGhoqHu70NBQOZ3Oi8YdDsclx3/Y/koCAxvJ29te1fLrlOBg\nv9ou4ZpCPzyPHnsePb466kqfqxzmP2az2WSz2WpiqkorKjrjkXmDPTKrZxUUlNR2CdeM4GA/+uFh\n9Njz6PHVYVqfL/fEo8qvZg8KClJ+fr4kKT8/X02bNpX0/RF3Xl6ee7u8vDw5HI6Lxp1O5yXHf9ge\nAABUTpXDPD4+XhkZGZKkjIwM9ezZs8K4ZVnavXu3/Pz8FBISotjYWG3btk0nT57UyZMntW3bNsXG\nxiokJES+vr7avXu3LMuqMBcAALiySp1mnzRpkj755BMVFRUpLi5O48ePV0pKiiZOnKjVq1erefPm\nWrBggSSpe/fu2rJlixISEtSwYUPNnTtXktSkSRONHTtWQ4cOlSSNGzdOTZo0kSTNmDFDU6dO1blz\n5xQXF6e4uDhPrBUAgDrJZhn6R92eus4RvMHfI/N6UkFCcW2XcM0w7RqYieix59Hjq8O0PnvkmjkA\nALg2EOa5D0tlAAAOqElEQVQAABiOMAcAwHCEOQAAhiPMAQAwHGEOAIDhCHMAAAxHmAMAYDjCHAAA\nwxHmAAAYjjAHAMBwhDkAAIYjzAEAMBxhDgCA4QhzAAAMR5gDAGA4whwAAMMR5gAAGI4wBwDAcIQ5\nAACGI8wBADAcYQ4AgOEIcwAADEeYAwBgOMIcAADDEeYAABiOMAcAwHCEOQAAhiPMAQAwHGEOAIDh\nCHMAAAznXdUdv/76az366KPu27m5uZowYYJKSkq0cuVKNW3aVJI0adIkde/eXZK0ZMkSrV69Wl5e\nXnrqqafUrVs3SVJ2draefvpplZeXKzk5WSkpKdVZEwAA15Uqh3nr1q21du1aSVJZWZni4uKUkJCg\n9PR0jRw5Ug8++GCF7Q8cOKDMzExlZmbK6XRq1KhR+vDDDyVJqampWr58uRwOh4YOHar4+Hjdeuut\n1VgWAADXjyqH+Y999NFHCgsLU4sWLX52m6ysLCUmJsrHx0dhYWFq1aqV9uzZI0lq1aqVwsLCJEmJ\niYnKysoizAEAqKQaCfPMzEz179/ffXvFihXKyMhQu3btNGXKFAUEBMjpdKpjx47ubRwOh5xOpyQp\nNDS0wvgPIX85gYGN5O1tr4nyjRcc7FfbJVxT6Ifn0WPPo8dXR13pc7XD3OVy6Z///Kcee+wxSdK9\n996rsWPHymaz6eWXX9a8efP0zDPPVLvQnyoqOlPjc0pSsEdm9ayCgpLaLuGaERzsRz88jB57Hj2+\nOkzr8+WeeFT71ezZ2dlq27atmjVrJklq1qyZ7Ha7vLy8lJycrL1790r6/og7Ly/PvZ/T6ZTD4fjZ\ncQAAUDnVDvPMzEwlJia6b+fn57u/3rhxo8LDwyVJ8fHxyszMlMvlUm5urnJyctShQwe1b99eOTk5\nys3NlcvlUmZmpuLj46tbFgAA141qnWY/c+aMduzYodTUVPfY888/r/3790uSWrRo4b4vPDxcffv2\nVb9+/WS32zV9+nTZ7d9f854+fbpGjx6tsrIyDRkyxP0EAAAAXJnNsiyrtouoCk9d5wje4O+ReT2p\nIKG4tku4Zph2DcxE9Njz6PHVYVqfPXrNHAAA1C7CHAAAwxHmAAAYjjAHAMBwhDkAAIYjzAEAMBxh\nDgCA4QhzAAAMR5gDAGA4whwAAMMR5gAAGI4wBwDAcIQ5AACGI8wBADAcYQ4AgOEIcwAADEeYAwBg\nOMIcAADDEeYAABiOMAcAwHCEOQAAhiPMAQAwHGEOAIDhvGu7AFx/gjf4e3Z+D8xZkFDsgVkBoGZw\nZA4AgOEIcwAADEeYAwBgOMIcAADDEeYAABiOMAcAwHCEOQAAhiPMAQAwXLXfNCY+Pl6NGzeWl5eX\n7Ha70tPT9d133+nRRx/V0aNH1aJFCy1YsEABAQGyLEtPP/20tmzZogYNGmjevHlq27atJGnNmjV6\n9dVXJUkPPfSQ7rnnnuqWBgDAdaFGjszffPNNrV27Vunp6ZKktLQ0RUdHa/369YqOjlZaWpokKTs7\nWzk5OVq/fr1mz56tmTNnSpK+++47LV68WCtXrtSqVau0ePFinTx5siZKAwCgzvPIafasrCwlJSVJ\nkpKSkrRx48YK4zabTZ06dVJxcbHy8/O1bds2xcTEqEmTJgoICFBMTIy2bt3qidIAAKhzauS92R98\n8EHZbDYNGzZMw4YNU2FhoUJCQiRJwcHBKiwslCQ5nU6Fhoa69wsNDZXT6bxo3OFwyOl0XvYxAwMb\nydvbXhPlGy842K+2S6jz6HFF9MPz6PHVUVf6XO0wf/vtt+VwOFRYWKhRo0apdevWFe632Wyy2WzV\nfZiLFBWdqfE5Jc98SIenFRSU1HYJvwg9NltwsB/98DB6fHWY1ufLPfGo9ml2h8MhSQoKClJCQoL2\n7NmjoKAg5efnS5Ly8/PVtGlT97Z5eXnuffPy8uRwOC4adzqd7nkBAMDlVSvMz5w5o1OnTrm/3r59\nu8LDwxUfH6+MjAxJUkZGhnr27ClJ7nHLsrR79275+fkpJCREsbGx2rZtm06ePKmTJ09q27Ztio2N\nrebSAAC4PlTrNHthYaHGjRsnSSorK1P//v0VFxen9u3ba+LEiVq9erWaN2+uBQsWSJK6d++uLVu2\nKCEhQQ0bNtTcuXMlSU2aNNHYsWM1dOhQSdK4cePUpEmT6pQGAMB1w2ZZllXbRVSFp65zBG/w98i8\nnlSQUFzbJfwi9Nhspl1nNBE9vjpM67NHr5kDAIDaRZgDAGA4whwAAMMR5gAAGI4wBwDAcIQ5AACG\nI8wBADAcYQ4AgOEIcwAADEeYAwBgOMIcAADDEeYAABiOMAcAwHCEOQAAhiPMAQAwHGEOAIDhCHMA\nAAxHmAMAYDjCHAAAwxHmAAAYzru2CwDgGcEb/D03twfmLEgo9sCswPWBI3MAAAxHmAMAYDjCHAAA\nwxHmAAAYjjAHAMBwhDkAAIYjzAEAMBxhDgCA4QhzAAAMR5gDAGC4Kof58ePHNWLECPXr10+JiYl6\n8803JUmLFi1St27dNGjQIA0aNEhbtmxx77NkyRIlJCSod+/e2rp1q3s8OztbvXv3VkJCgtLS0qqx\nHAAArj9Vfm92u92uKVOmqG3btjp16pSGDBmimJgYSdLIkSP14IMPVtj+wIEDyszMVGZmppxOp0aN\nGqUPP/xQkpSamqrly5fL4XBo6NChio+P16233lqNZQEAcP2ocpiHhIQoJCREkuTr66vWrVvL6XT+\n7PZZWVlKTEyUj4+PwsLC1KpVK+3Zs0eS1KpVK4WFhUmSEhMTlZWVRZgDAFBJNfKpaUeOHNFXX32l\njh076l//+pdWrFihjIwMtWvXTlOmTFFAQICcTqc6duzo3sfhcLjDPzQ0tML4DyF/OYGBjeTtba+J\n8o0XHOxX2yXUefTY8+hxRfTj6qgrfa52mJ8+fVoTJkzQtGnT5Ovrq3vvvVdjx46VzWbTyy+/rHnz\n5umZZ56piVorKCo6U+NzSp75aEdPKygoqe0SfhF6fHWY1mcTe+wpwcF+9OMqMK3Pl3viUa1Xs58/\nf14TJkzQgAED1KtXL0lSs2bNZLfb5eXlpeTkZO3du1fS90fceXl57n2dTqccDsfPjgMAgMqpcphb\nlqUnn3xSrVu31qhRo9zj+fn57q83btyo8PBwSVJ8fLwyMzPlcrmUm5urnJwcdejQQe3bt1dOTo5y\nc3PlcrmUmZmp+Pj4aiwJAIDrS5VPs3/22Wdau3atbrvtNg0aNEiSNGnSJK1bt0779++XJLVo0UKp\nqamSpPDwcPXt21f9+vWT3W7X9OnTZbd/f817+vTpGj16tMrKyjRkyBD3EwAAAHBlNsuyrNouoio8\ndZ0jeIO/R+b1pIKE4tou4Rehx1eHaX02sceeYtq1XFOZ1mePXTMHAAC1jzAHAMBwhDkAAIYjzAEA\nMBxhDgCA4QhzAAAMR5gDAGA4whwAAMMR5gAAGI4wBwDAcDXyeeYAcD3y5FvmeuojbHnb3LqJI3MA\nAAxHmAMAYDjCHAAAwxHmAAAYjjAHAMBwhDkAAIYjzAEAMBxhDgCA4QhzAAAMR5gDAGA4whwAAMMR\n5gAAGI4wBwDAcHxqGgDgmuXJT6aTPPPpdLXxyXQcmQMAYDjCHAAAwxHmAAAYjjAHAMBwhDkAAIYj\nzAEAMNw1E+bZ2dnq3bu3EhISlJaWVtvlAABgjGsizMvKypSamqqlS5cqMzNT69at04EDB2q7LAAA\njHBNhPmePXvUqlUrhYWFycfHR4mJicrKyqrtsgAAMMI18Q5wTqdToaGh7tsOh0N79uy57D7BwX6e\nKeY+yzPzepAn3sHIo+jx1WFYn+nx1WFcn+lxpVwTR+YAAKDqrokwdzgcysvLc992Op1yOBy1WBEA\nAOa4JsK8ffv2ysnJUW5urlwulzIzMxUfH1/bZQEAYIRr4pq5t7e3pk+frtGjR6usrExDhgxReHh4\nbZcFAIARbJZlmffqAgAA4HZNnGYHAABVR5gDAGA4whwAAMMR5gBQy/bs2eN+o6wDBw5o+fLl2rJl\nSy1XVbf94Q9/qO0SahQvgEOdsWvXLu3du1fh4eGKjY2t7XLqjIMHDyo/P18dOnRQ48aN3ePZ2dmK\ni4urxcrqhsWLFys7O1sXLlxQTEyMPv/8c0VFRWnHjh2KjY3VQw89VNslGm/MmDEXje3cuVNRUVGS\npNdee+1ql1Tjrok/TbvevPfeexoyZEhtl2G8oUOHavXq1ZKklStXasWKFUpISNDixYv15ZdfKiUl\npZYrNN9bb72lFStW6JZbbtH+/fs1bdo03X333ZKkl156iTCvAR9++KEyMjLkcrkUExOj7Oxs+fr6\n6sEHH1RycjJhXgOcTqduueUWJScny2azybIs7du3Tw888EBtl1ZjOM1eCxYtWlTbJdQJFy5ccH/9\n7rvvavny5Xr44Yf1+uuv6/3336/FyuqOVatWKT09Xa+88oreeustvfLKK3rzzTclSZzUqxl2u112\nu10NGzZUy5Yt5evrK0lq0KCBvLz4FV0T3nvvPbVr106vvfaa/Pz8FBUVpfr166tr167q2rVrbZdX\nIzgy95ABAwb87H3ffvvtVayk7iovL9fJkydVXl4uy7LUtGlTSVKjRo1kt9trubq6oby83H1q/cYb\nb9Rf/vIXTZgwQceOHSPMa0i9evV09uxZNWzYUOnp6e7xkpISwryGeHl5aeTIkerTp4/mzp2rZs2a\nqaysrLbLqlGEuYcUFhZq2bJl8vf3rzBuWZaGDx9eS1XVLadOndLgwYNlWZZsNpvy8/MVEhKi06dP\nEzQ1JCgoSF999ZXatGkjSWrcuLGWLFmiadOm6d///nctV1c3rFixQj4+PpJUIbzPnz+vefPm1VZZ\ndVJoaKgWLlyozZs3u8+A1BW8AM5Dpk2bpsGDBysiIuKi+x577DG9+OKLtVDV9eHs2bP69ttvFRYW\nVtulGC8vL092u13BwRd/qONnn32mO+64oxaqAvBThDkAAIbjggwAAIYjzAEAMBxhDgCA4QhzAAAM\n938Bc3Z59pQk6qAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6b15de5550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.mark.value_counts().plot(kind='bar', label='mark', color='orange')\n",
    "plt.legend(fontsize=15)\n",
    "plt.title('Amount of each mark in the sample data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be critical for classifier to have such a large number of one class and a little number of other classes, so we remember it and may do oversampling later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can try different approaches: classification with 5 classes, or regression with target 'mark'.\n",
    "### Let`s try classification first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before classification or regression we must extract features from the data, I`ll use **TfidfVectorizer** (because usually it is better than regular *bag-of-words*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.74 s, sys: 70 ms, total: 5.81 s\n",
      "Wall time: 5.88 s\n",
      "(28916, 134520)\n"
     ]
    }
   ],
   "source": [
    "X = data['sentiment']\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
    "\n",
    "%time X = vectorizer.fit_transform(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a data to train our models. Let`s try all the classifiers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1). Linear models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data['mark']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01 \t accuracy=0.6105 +-0.0002\n",
      "C=0.1 \t accuracy=0.7511 +-0.0035\n",
      "C=0.5 \t accuracy=0.7799 +-0.0028\n",
      "C=1 \t accuracy=0.7844 +-0.0025\n",
      "C=5 \t accuracy=0.7832 +-0.0009\n",
      "C=10 \t accuracy=0.7791 +-0.0012\n",
      "C=100 \t accuracy=0.7643 +-0.0014\n",
      "C=200 \t accuracy=0.7612 +-0.0009\n",
      "C=500 \t accuracy=0.7572 +-0.0013\n",
      "C=1000 \t accuracy=0.7556 +-0.0015\n",
      "C=10000 \t accuracy=0.7504 +-0.0007\n",
      "C=15000 \t accuracy=0.7504 +-0.0010\n",
      "C=20000 \t accuracy=0.7501 +-0.0007\n",
      "C=100000 \t accuracy=0.7485 +-0.0021\n",
      "CPU times: user 10min 5s, sys: 547 ms, total: 10min 6s\n",
      "Wall time: 10min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_C = -1\n",
    "for cur_C in [0.01, 0.1, 0.5, 1, 5, 10, 100, 200, 500, 1000, 10000, 15000, 20000, 100000]:\n",
    "    cls = LogisticRegression(C=cur_C)\n",
    "    cv_scores = cross_val_score(cls, X, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_C = cur_C\n",
    "    print('C={0}'.format(cur_C), '\\t', \n",
    "          'accuracy={0:0.4f}'.format(cur_avg_acc), \n",
    "          '+-{0:0.4f}'.format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are: C=1 with accuracy=0.7844\n"
     ]
    }
   ],
   "source": [
    "print('Best params are: C={0} with accuracy={1:0.4f}'.format(best_C, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StochasticGradientDescentClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.0001, power_t=0.1 \t accuracy=0.7874 +-0.0024\n",
      "alpha=0.0001, power_t=0.5 \t accuracy=0.7871 +-0.0028\n",
      "alpha=0.0001, power_t=1 \t accuracy=0.7874 +-0.0023\n",
      "alpha=0.0001, power_t=1.5 \t accuracy=0.7875 +-0.0025\n",
      "alpha=0.0001, power_t=2 \t accuracy=0.7872 +-0.0031\n",
      "alpha=0.0001, power_t=3 \t accuracy=0.7871 +-0.0032\n",
      "alpha=0.0001, power_t=5 \t accuracy=0.7873 +-0.0027\n",
      "alpha=0.0001, power_t=10 \t accuracy=0.7871 +-0.0030\n",
      "alpha=0.0001, power_t=100 \t accuracy=0.7873 +-0.0025\n",
      "alpha=0.0001, power_t=500 \t accuracy=0.7875 +-0.0021\n",
      "alpha=0.0001, power_t=1000 \t accuracy=0.7870 +-0.0024\n",
      "alpha=0.0005, power_t=0.1 \t accuracy=0.7706 +-0.0033\n",
      "alpha=0.0005, power_t=0.5 \t accuracy=0.7703 +-0.0031\n",
      "alpha=0.0005, power_t=1 \t accuracy=0.7707 +-0.0034\n",
      "alpha=0.0005, power_t=1.5 \t accuracy=0.7708 +-0.0030\n",
      "alpha=0.0005, power_t=2 \t accuracy=0.7704 +-0.0028\n",
      "alpha=0.0005, power_t=3 \t accuracy=0.7711 +-0.0036\n",
      "alpha=0.0005, power_t=5 \t accuracy=0.7709 +-0.0032\n",
      "alpha=0.0005, power_t=10 \t accuracy=0.7703 +-0.0031\n",
      "alpha=0.0005, power_t=100 \t accuracy=0.7703 +-0.0027\n",
      "alpha=0.0005, power_t=500 \t accuracy=0.7716 +-0.0033\n",
      "alpha=0.0005, power_t=1000 \t accuracy=0.7704 +-0.0028\n",
      "alpha=0.001, power_t=0.1 \t accuracy=0.7512 +-0.0036\n",
      "alpha=0.001, power_t=0.5 \t accuracy=0.7513 +-0.0036\n",
      "alpha=0.001, power_t=1 \t accuracy=0.7508 +-0.0041\n",
      "alpha=0.001, power_t=1.5 \t accuracy=0.7508 +-0.0038\n",
      "alpha=0.001, power_t=2 \t accuracy=0.7509 +-0.0039\n",
      "alpha=0.001, power_t=3 \t accuracy=0.7509 +-0.0041\n",
      "alpha=0.001, power_t=5 \t accuracy=0.7508 +-0.0038\n",
      "alpha=0.001, power_t=10 \t accuracy=0.7510 +-0.0036\n",
      "alpha=0.001, power_t=100 \t accuracy=0.7509 +-0.0040\n",
      "alpha=0.001, power_t=500 \t accuracy=0.7509 +-0.0037\n",
      "alpha=0.001, power_t=1000 \t accuracy=0.7511 +-0.0041\n",
      "alpha=0.01, power_t=0.1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=0.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=1.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=2 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=3 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=10 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=100 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=500 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.01, power_t=1000 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=0.1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=0.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=1.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=2 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=3 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=10 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=100 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=500 \t accuracy=0.6104 +-0.0001\n",
      "alpha=0.1, power_t=1000 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=0.1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=0.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=1.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=2 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=3 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=10 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=100 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=500 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1, power_t=1000 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=0.1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=0.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=1.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=2 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=3 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=10 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=100 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=500 \t accuracy=0.6104 +-0.0001\n",
      "alpha=2, power_t=1000 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=0.1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=0.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=1.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=2 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=3 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=10 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=100 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=500 \t accuracy=0.6104 +-0.0001\n",
      "alpha=4, power_t=1000 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=0.1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=0.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=1.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=2 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=3 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=10 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=100 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=500 \t accuracy=0.6104 +-0.0001\n",
      "alpha=5, power_t=1000 \t accuracy=0.6104 +-0.0001\n",
      "alpha=10, power_t=0.1 \t accuracy=0.5510 +-0.1188\n",
      "alpha=10, power_t=0.5 \t accuracy=0.6206 +-0.0202\n",
      "alpha=10, power_t=1 \t accuracy=0.6104 +-0.0001\n",
      "alpha=10, power_t=1.5 \t accuracy=0.6104 +-0.0001\n",
      "alpha=10, power_t=2 \t accuracy=0.6104 +-0.0001\n",
      "alpha=10, power_t=3 \t accuracy=0.6104 +-0.0001\n",
      "alpha=10, power_t=5 \t accuracy=0.6216 +-0.0137\n",
      "alpha=10, power_t=10 \t accuracy=0.6104 +-0.0001\n",
      "alpha=10, power_t=100 \t accuracy=0.6104 +-0.0001\n",
      "alpha=10, power_t=500 \t accuracy=0.6104 +-0.0001\n",
      "alpha=10, power_t=1000 \t accuracy=0.6104 +-0.0001\n",
      "CPU times: user 4min 9s, sys: 36.7 ms, total: 4min 9s\n",
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_alpha = -1\n",
    "best_t = -1\n",
    "for cur_alpha, cur_t in itertools.product([0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 2, 4, 5, 10],\n",
    "                                          [0.1, 0.5, 1, 1.5, 2, 3, 5, 10, 100, 500, 1000]):\n",
    "    cls = SGDClassifier(alpha=cur_alpha, power_t=cur_t)\n",
    "    cv_scores = cross_val_score(cls, X, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_alpha = cur_alpha\n",
    "        best_t = cur_t\n",
    "    print('alpha={0}, power_t={1}'.format(cur_alpha, cur_t), '\\t', \n",
    "          'accuracy={0:0.4f}'.format(cur_avg_acc), \n",
    "          '+-{0:0.4f}'.format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are: alpha=0.0001, power_t=1.5 with accuracy=0.7875\n"
     ]
    }
   ],
   "source": [
    "print('Best params are: alpha={0}, power_t={1} with accuracy={2:0.4f}'.format(best_alpha, best_t, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01 \t accuracy=0.7700 +-0.0031\n",
      "C=0.1 \t accuracy=0.7895 +-0.0022\n",
      "C=0.5 \t accuracy=0.7674 +-0.0045\n",
      "C=1 \t accuracy=0.7556 +-0.0023\n",
      "C=5 \t accuracy=0.7489 +-0.0046\n",
      "C=10 \t accuracy=0.7454 +-0.0048\n",
      "C=100 \t accuracy=0.7472 +-0.0056\n",
      "C=200 \t accuracy=0.7441 +-0.0057\n",
      "C=500 \t accuracy=0.7500 +-0.0024\n",
      "C=1000 \t accuracy=0.7472 +-0.0033\n",
      "C=10000 \t accuracy=0.7444 +-0.0053\n",
      "C=15000 \t accuracy=0.7421 +-0.0043\n",
      "C=20000 \t accuracy=0.7454 +-0.0069\n",
      "C=100000 \t accuracy=0.7451 +-0.0066\n",
      "CPU times: user 31 s, sys: 0 ns, total: 31 s\n",
      "Wall time: 31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_C = -1\n",
    "for cur_C in [0.01, 0.1, 0.5, 1, 5, 10, 100, 200, 500, 1000, 10000, 15000, 20000, 100000]:\n",
    "    cls = PassiveAggressiveClassifier(C=cur_C)\n",
    "    cv_scores = cross_val_score(cls, X, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_C = cur_C\n",
    "    print('C={0}'.format(cur_C), '\\t', \n",
    "          'accuracy={0:0.4f}'.format(cur_avg_acc), \n",
    "          '+-{0:0.4f}'.format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are: C=0.1 with accuracy=0.7895\n"
     ]
    }
   ],
   "source": [
    "print('Best params are: C={0} with accuracy={1:0.4f}'.format(best_C, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.01 \t accuracy=0.7129 +-0.0064\n",
      "alpha=0.05 \t accuracy=0.7332 +-0.0061\n",
      "alpha=0.1 \t accuracy=0.7473 +-0.0040\n",
      "alpha=0.5 \t accuracy=0.7783 +-0.0016\n",
      "alpha=1 \t accuracy=0.7840 +-0.0017\n",
      "alpha=1.5 \t accuracy=0.7864 +-0.0029\n",
      "alpha=3 \t accuracy=0.7859 +-0.0026\n",
      "alpha=5 \t accuracy=0.7848 +-0.0025\n",
      "alpha=10 \t accuracy=0.7808 +-0.0027\n",
      "alpha=100 \t accuracy=0.7147 +-0.0034\n",
      "alpha=250 \t accuracy=0.6306 +-0.0011\n",
      "alpha=500 \t accuracy=0.6104 +-0.0001\n",
      "alpha=1000 \t accuracy=0.6104 +-0.0001\n",
      "CPU times: user 6min 22s, sys: 983 ms, total: 6min 23s\n",
      "Wall time: 6min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_alpha = -1\n",
    "for cur_alpha in [0.01,0.05, 0.1, 0.5, 1, 1.5, 3, 5, 10, 100, 250, 500, 1000]:\n",
    "    cls = RidgeClassifier(alpha=cur_alpha)\n",
    "    cv_scores = cross_val_score(cls, X, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_alpha = cur_alpha\n",
    "    print('alpha={0}'.format(cur_alpha), '\\t', \n",
    "          'accuracy={0:0.4f}'.format(cur_avg_acc), \n",
    "          '+-{0:0.4f}'.format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are: alpha=1.5 with accuracy=0.7844\n"
     ]
    }
   ],
   "source": [
    "print('Best params are: alpha={0} with accuracy={1:0.4f}'.format(best_alpha, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The winner among the linear models - PassiveAggressiveClassifier with accuracy=0.7895"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2). Bayesian Classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.8 s, sys: 143 ms, total: 9.94 s\n",
      "Wall time: 9.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_alpha = -1\n",
    "for cur_alpha in [0.01, 0.05, 0.1, 0.5, 1, 1.5, 3, 5, 10, 100, 250, 500]:\n",
    "    clf = BernoulliNB(alpha=cur_alpha)\n",
    "    cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_alpha = cur_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are: alpha=3 with accuracy=0.7262\n"
     ]
    }
   ],
   "source": [
    "print('Best params are: alpha={0} with accuracy={1:0.4f}'.format(best_alpha, best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.94 s, sys: 40 ms, total: 5.98 s\n",
      "Wall time: 5.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_alpha = -1\n",
    "grid = np.linspace(0.01, 0.1, 10)\n",
    "for cur_alpha in grid:  # [0.01, 0.05, 0.1, 0.5, 1, 1.5, 3, 5, 10, 100, 250, 500]:\n",
    "    clf = MultinomialNB(alpha=cur_alpha)\n",
    "    cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_alpha = cur_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are: alpha=0.05000000000000001 with accuracy=0.7658\n"
     ]
    }
   ],
   "source": [
    "print('Best params are: alpha={0} with accuracy={1:0.4f}'.format(best_alpha, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3). Metric Classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min, sys: 7.53 s, total: 3min 7s\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_k = -1\n",
    "for cur_k in [10, 25, 50, 100]:  # [1, 3, 5] have already given worse result\n",
    "    clf = KNeighborsClassifier(n_neighbors=cur_k)\n",
    "    cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_k = cur_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are: k=25 with accuracy=0.7550\n"
     ]
    }
   ],
   "source": [
    "print('Best params are: k={0} with accuracy={1:0.4f}'.format(best_k, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4). Ensembles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.749654463571\n",
      "CPU times: user 6min 16s, sys: 26.7 ms, total: 6min 16s\n",
      "Wall time: 6min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = AdaBoostClassifier()\n",
    "cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=5)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.746783227622\n",
      "CPU times: user 46min 12s, sys: 547 ms, total: 46min 13s\n",
      "Wall time: 46min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = BaggingClassifier()\n",
    "cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=5)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.737860450952\n",
      "CPU times: user 1min 4s, sys: 6.67 ms, total: 1min 4s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = ExtraTreesClassifier()\n",
    "cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=5)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.737446439707\n",
      "CPU times: user 39.8 s, sys: 3.33 ms, total: 39.8 s\n",
      "Wall time: 39.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = RandomForestClassifier()\n",
    "cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=5)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1). Gradient Boosting Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "# import lightgbm as lgb  ## can`t install :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-d22c237c4e12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clf = GradientBoostingClassifier()\\ncv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=3)\\nprint(cv_scores.mean())'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2101\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2103\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2104\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m                                               fit_params)\n\u001b[0;32m-> 1433\u001b[0;31m                       for train, test in cv)\n\u001b[0m\u001b[1;32m   1434\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateComputeBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m         \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m             \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1606\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m         raise ValueError(\"scoring must return a number, got %s (%s) instead.\"\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mScore\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mapplied\u001b[0m \u001b[0mto\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0mof\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0mon\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1496\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m         \"\"\"\n\u001b[0;32m-> 1498\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1499\u001b[0m         \u001b[0mdecisions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_score_to_decision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecisions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1454\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m         \"\"\"\n\u001b[0;32m-> 1456\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1457\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n\u001b[0;32m--> 371\u001b[0;31m                                       force_all_finite)\n\u001b[0m\u001b[1;32m    372\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \"\"\"\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[1;32m    239\u001b[0m                         \u001b[0;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                         'convert to a dense numpy array.')\n",
      "\u001b[0;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = GradientBoostingClassifier()\n",
    "cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=3)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, I have some problems with *sklearns GBClf* and *LightGBM clf*, later I'll fix it, but now let`s try **XGBoost**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.757746536539\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "cv_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=3)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let`s try to make a **bigrams** and **trigrams** on **pre-cleaned with nltk** data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#nltk.download()  # uncomment if you use nltk for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кружк\n",
      "чашк\n",
      "бегущ\n",
      "дела\n"
     ]
    }
   ],
   "source": [
    "stemmer = RussianStemmer()\n",
    "print(stemmer.stem(\"Кружки\"))\n",
    "print(stemmer.stem(\"Чашки\"))\n",
    "print(stemmer.stem(\"бегущий\"))\n",
    "print(stemmer.stem(\"делающий\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кружки\n",
      "бегущий\n",
      "анклавы\n",
      "лучше\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"Кружки\"))\n",
    "print(lemmatizer.lemmatize(\"бегущий\"))\n",
    "print(lemmatizer.lemmatize(\"анклавы\"))\n",
    "print(lemmatizer.lemmatize(\"лучше\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we can see **stemming** and **lemmatization** for russian language in **nltk** is a bit **'strange'**.  \n",
    "Let`s try **pymorphy** (https://pymorphy2.readthedocs.io/en/latest/user/guide.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='стали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='стать', score=0.984662, methods_stack=((<DictionaryAnalyzer>, 'стали', 904, 4),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 1),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 2),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 5),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 6),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 9),))]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "morph.parse('стали')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бежать кружка анклав молодец хороший\n"
     ]
    }
   ],
   "source": [
    "print(morph.parse('бегущий')[0].normal_form,\n",
    "      morph.parse('кружки')[0].normal_form,\n",
    "      morph.parse('Анклавы')[0].normal_form,\n",
    "      morph.parse('молодец')[0].normal_form,\n",
    "      morph.parse('лучше')[0].normal_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that pymorphy handles with russian very good, so now I am going to:\n",
    "- apply **.parse().normal_form** to corpus of sentiments;\n",
    "- make **tf-idf** for this corpus with **bigrams and trigrams**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with pymorpy2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я имею кредитную карту. Пользуюсь ею длительное время. Ни разу не пожалела об этом! Возникающие вопросы направляла в банк и в течение одного дня получала квалифицированные ответы. Отзывчивость, доброжелательность и индивидуальный подход - особенность этого банка. Ни один мой вопрос не остался без ответа. В рамках этого отзыва не могу описать возникшие у меня вопросы. Достаточно сказать, что надежды на их решение у меня не было даже в принципе. Тем не менее, сотрудники банка решили все мои проблемы. Тинькофф банк - это современный прогрессивный банк, ценящий своих клиентов, имеет огромный потенциал в виде высокопрофессиональных сотрудников. Буду рекомендовать друзьям и знакомым!\n"
     ]
    }
   ],
   "source": [
    "X = data['sentiment']\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Tokenizing with nltk**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет!', 'Меня зовут Илья, а тебя?', ':)']\n",
      "['Привет', '!', 'Меня', 'зовут', 'Илья', ',', 'а', 'тебя', '?', ':', ')']\n"
     ]
    }
   ],
   "source": [
    "text = \"Привет! Меня зовут Илья, а тебя? :)\"\n",
    "\n",
    "print(sent_tokenize(text))\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-cba92d0e36fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Привет! Меня зовут Илья, а тебя? :)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Тут я хочу'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'протестировать токенизацию'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'на несколких предлжениях.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[1;32m    110\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[1;32m     93\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m    309\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "text = ['Привет! Меня зовут Илья, а тебя? :)', 'Тут я хочу', 'протестировать токенизацию', 'на несколких предлжениях.']\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*word_tokenize()* doesn`t work with lists of strings, so I must loop over all the corpus and than loop over all the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['привет', '!', 'я', 'звать', 'илья', ',', 'а', 'ты', '?', ':', ')']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[morph.parse(token)[0].normal_form for token in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pymorphy_lemmatize(sentiment):\n",
    "    '''Returns new, lemmatized sentiment'''\n",
    "    \n",
    "    tokenized_sentiment = word_tokenize(sentiment)\n",
    "    new_sentiment = ''\n",
    "    return ' '.join([morph.parse(token)[0].normal_form for token in tokenized_sentiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'привет ! это тест функция , который токенизировать и лемматизировать отзыв . : )'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pymorphy_lemmatize('Привет! Это тест функции, которая токенизирует и лемматизирует отзывы. :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pymorphy_transform(corpus):\n",
    "    '''Returns corpus of lemmatized sentiments'''\n",
    "    \n",
    "    return [pymorphy_lemmatize(sentiment) for sentiment in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'Я имею кредитную карту. Пользуюсь ею длительное время. Ни разу не пожалела об этом! Возникающие вопросы направляла в банк и в течение одного дня получала квалифицированные ответы. Отзывчивость, доброжелательность и индивидуальный подход - особенность этого банка. Ни один мой вопрос не остался без ответа. В рамках этого отзыва не могу описать возникшие у меня вопросы. Достаточно сказать, что надежды на их решение у меня не было даже в принципе. Тем не менее, сотрудники банка решили все мои проблемы. Тинькофф банк - это современный прогрессивный банк, ценящий своих клиентов, имеет огромный потенциал в виде высокопрофессиональных сотрудников. Буду рекомендовать друзьям и знакомым!'\n",
      " 'Всем привет! Я в этом банке , как только рухнул СВЯЗНОЙ БАНК , очень доволен обслуживанием и функционалом, если и возникали проблемы, то были решаемы очень быстро по телефону в общении со специалистами банка, всеми консультациями доволен, работу ребята делают на твердую ПЯТЕРКУ!!! Вот и сегодня меня по моей собственной ошибке обманули, я сразу в тех поддержку, карту заблокировали, проконсультировали, тут же оформили новую карту, при чем \\xa0, сотрудник банка Алексей ( вн. номер 61145) очень быстро решил мою проблему, \\xa0терпеливо, вежливо и доходчиво объяснил мне в чем я совершил ошибку и организовал мне близлежащую встречу с сотрудником банка (курьером) \\xa0, который мне и передаст мою новую дебетовую карту. Многие мои друзья так же , как и я пользуются услугами Тинькофф банка по моей рекомендации и очень довольны: проценты, кешбек, обслуживание. Так же радует акция \" приведи друга\" на днях снова получил свои 750 рублей.\\xa0p.s\\xa0Если руководство банка читает мой отзыв, прошу вас поощрить вашего сотрудника Алексея (вн. номер 61145)'\n",
      " 'Добрый вечер.Был вашим вкладчиком на протяжении 2015-2016г,\\xa0 мне предложили \\xa0в\\xa0 вашембанке отделение Красная Пресня воспользоваться кредитной картой, я получил\\xa0 золотую карту MASTERCARD. На протяжении года я ей пользовалсяуспешно, до недавнего времени.1.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0При открытии карты меня не кто не предупредил,что мне подключили услугу ЗАЩИТА КАРТЫ, (Я НЕ ПОДПИСЫВАЛ ДОКУМЕНТЫ НАПОДКЛЮЧЕНИЕ ДАННОЙ УСЛУГИ) за эту услугу ВАШ БАНК\\xa0 списывал с моей карты денежные средства. \\xa0Жалобу на необоснованное списания я сегодня 16ноября 2016 года в отделение Красная Пресня я оформил №1-1587039. ПРОШУ ВЕРНУТЬДЕНЬГИ В ПОЛНОМ ОБЪЕМЕ2.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Яделал перевод с карты БИНБАНКА\\xa0 на другуюкарту ТИНЬКОФ как покупку\\xa0 на протяжениивсего пользования, никогда комиссий не было, но недавно, а именно 22 октября2016г оформил покупку на банк ТИНЬКОФ 40000 рублей и у меня списали комиссию вразмере 1554,11 руб, я пришел в отделение на Красной Пресне и оформилпретензию\\xa0 за № 1-10674636573 от26.10.2016 ответ так и не поступил. Прочее комиссии списанные с меня по карте\\xa0 в ноябре в размере 1,500 руб также никто неможет объяснить(ЗА ЧТО СПИСАЛИ С МЕНЯ ЭТИ ДЕНЕЖНЫЕ СРЕДСТВА)\\xa0 ПРОШУ ВЕРНУТЬ ДЕНЬГИ В ПОЛНОМ ОБЪЕМЕ3.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Неоднократно звонил в ВАШ КОЛЛ-Центр,адекватного или удовлетворительно ответа от ваших сотрудников\\xa0 разъяснить мне\\xa0\\xa0 на каком основании с меня были списаныденежные средства я\\xa0\\xa0 не получил!!! \\xa016ноября 2016г я закрыл вашу кредитную карту,в связи с утратой моего терпения и\\xa0доверия как клиента, как вкладчика к Вашему Банку. В случае непринятиямер оставляю за собой право обратиться в контролирующие органы Банка России.4.\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Прошел месяц с моей первой жалобы ответа от вашегобанка\\xa0 я так и не получил!!!'\n",
      " 'Очень разочарована банком ВТБ24. За смс уведомление деньги банк снимает вовремя, а сами уведомления,видимо,пешком идут. И виноват не банк,как сообщил сотрудник, а оператор связи! Видимо, совсем плохи дела у банка, т.к деньги, поступившие на счет кредитной карты, внимание!!! должны сутки побыть на карте, в противном случае их система не засчитывает! Это как надо неуважительно относиться к клиенту!'\n",
      " 'Отвратительный банк, и обслуживание. 24.11.2016 зарегистрировал киви Кошелек и его полностью идентифицировал, так на следующий день мне его заблокировали с суммой 19к рублей. отправил 2 запроса на разблокировку киви кошелька, так СБ и тех поддержка просто морозиться на мои сообщения отправлены мною им на почту. только пришло авто сообщение что моя заявка зарегистрирована и все.Прошу разобраться с этим вопросом номер тикета\\xa0№ 6626433.через форму на сайте были отправлены все потребываемые документыchernovalov.shura@mail.ruchernovalov.shura@mail.ru']\n"
     ]
    }
   ],
   "source": [
    "test = X[0:5].values\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pymorphy_transform(test))\n",
    "print(len(pymorphy_transform(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!! ATTENTION !!!**: next cell executes more than 20 minutes, be careful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_lemmatized = pymorphy_transform(X.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the new, lemmatized data to the .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = './data/lemmatized_responses_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatized_data = pd.DataFrame({'mark': data['mark'], 'sentiment': X_lemmatized})\n",
    "lemmatized_data.to_csv(file_name, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading new data** (made for testing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>mark</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>я иметь кредитный карта . пользоваться она дли...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>весь привет ! я в это банка , как только рухну...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>добрый вечер.быть ваш вкладчик на протяжение 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>очень разочаровать банк втб24 . за смс уведомл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>отвратительный банк , и обслуживание . 24.11.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  mark                                          sentiment\n",
       "0           0     5  я иметь кредитный карта . пользоваться она дли...\n",
       "1           1     5  весь привет ! я в это банка , как только рухну...\n",
       "2           2     1  добрый вечер.быть ваш вкладчик на протяжение 2...\n",
       "3           3     1  очень разочаровать банк втб24 . за смс уведомл...\n",
       "4           4     1  отвратительный банк , и обслуживание . 24.11.2..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_data = pd.read_csv(file_name, sep='\\t')\n",
    "lemmatized_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28916"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemmatized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let`s drop odd column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'mark', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatized_data.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mark</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>я иметь кредитный карта . пользоваться она дли...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>весь привет ! я в это банка , как только рухну...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>добрый вечер.быть ваш вкладчик на протяжение 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>очень разочаровать банк втб24 . за смс уведомл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>отвратительный банк , и обслуживание . 24.11.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mark                                          sentiment\n",
       "0     5  я иметь кредитный карта . пользоваться она дли...\n",
       "1     5  весь привет ! я в это банка , как только рухну...\n",
       "2     1  добрый вечер.быть ваш вкладчик на протяжение 2...\n",
       "3     1  очень разочаровать банк втб24 . за смс уведомл...\n",
       "4     1  отвратительный банк , и обслуживание . 24.11.2..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatized_data.to_csv(file_name, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Now we have dataset of lemmatized sentiments, let`s train Tf*Idf with bigrams and then re-train our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28916, 1357308)\n",
      "CPU times: user 22.6 s, sys: 413 ms, total: 23 s\n",
      "Wall time: 23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1, 2))\n",
    "\n",
    "X_lemmatized = lemmatized_data['sentiment']\n",
    "X_transformed = vectorizer.fit_transform(X_lemmatized)\n",
    "print(X_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = lemmatized_data['mark']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PassiveAgressiveClassifier (previous result - accuracy = 0.7895)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01 \t accuracy=0.7368 +-0.0040\n",
      "C=0.1 \t accuracy=0.7889 +-0.0017\n",
      "C=0.5 \t accuracy=0.7891 +-0.0012\n",
      "C=1 \t accuracy=0.7887 +-0.0014\n",
      "C=5 \t accuracy=0.7888 +-0.0017\n",
      "C=10 \t accuracy=0.7890 +-0.0019\n",
      "C=100 \t accuracy=0.7892 +-0.0017\n",
      "C=200 \t accuracy=0.7893 +-0.0019\n",
      "C=500 \t accuracy=0.7890 +-0.0011\n",
      "C=1000 \t accuracy=0.7890 +-0.0019\n",
      "C=10000 \t accuracy=0.7889 +-0.0017\n",
      "C=15000 \t accuracy=0.7889 +-0.0016\n",
      "C=20000 \t accuracy=0.7893 +-0.0013\n",
      "C=100000 \t accuracy=0.7891 +-0.0014\n",
      "CPU times: user 1min 56s, sys: 5.91 s, total: 2min 1s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_C = -1\n",
    "for cur_C in [0.01, 0.1, 0.5, 1, 5, 10, 100, 200, 500, 1000, 10000, 15000, 20000, 100000]:\n",
    "    cls = PassiveAggressiveClassifier(C=cur_C)\n",
    "    cv_scores = cross_val_score(cls, X_transformed, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_C = cur_C\n",
    "    print('C={0}'.format(cur_C), '\\t', \n",
    "          'accuracy={0:0.4f}'.format(cur_avg_acc), \n",
    "          '+-{0:0.4f}'.format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params are: C=20000 with accuracy=0.7893\n"
     ]
    }
   ],
   "source": [
    "print('Best params are: C={0} with accuracy={1:0.4f}'.format(best_C, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, I have worse result, than before lemmatization, let`s try SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "best_acc = -1\n",
    "best_C = -1\n",
    "for cur_C in [0.01]:#, 0.1, 0.5, 1, 5, 10, 100, 200, 500, 1000, 10000]:\n",
    "    cls = SVC(C=cur_C)\n",
    "    cv_scores = cross_val_score(cls, X_transformed, y, scoring=\"accuracy\", cv=5)\n",
    "    cur_avg_acc = np.mean(cv_scores)\n",
    "    if cur_avg_acc > best_acc:\n",
    "        best_acc = cur_avg_acc\n",
    "        best_C = cur_C\n",
    "    print('C={0}'.format(cur_C), '\\t', \n",
    "          'accuracy={0:0.4f}'.format(cur_avg_acc), \n",
    "          '+-{0:0.4f}'.format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Best params are: C={0} with accuracy={1:0.4f}'.format(best_C, best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.764040807058\n",
      "CPU times: user 48min 1s, sys: 8.18 s, total: 48min 9s\n",
      "Wall time: 10min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = xgb.XGBClassifier()\n",
    "cv_scores = cross_val_score(clf, X_transformed, y, scoring=\"accuracy\", cv=3)\n",
    "print(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous result was 0.7577, so we have improved the model, parameter tuning can make > 0.8 accuracy, but for now that`s all. \n",
    "\n",
    "I am going to save best models and vectorizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainig final Vectorizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data['sentiment']\n",
    "\n",
    "best_vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
    "\n",
    "X = best_vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134520"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./vectorizers/best_vectorizer_vocabulary.pkl', 'wb') as vocab_file:\n",
    "    _pickle.dump(best_vectorizer.vocabulary_, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#joblib.dump(best_vectorizer.vocabulary_, './vectorizers/best_vectorizer_vocabulary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data['mark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassiveAggressiveClassifier(C=0.1, class_weight=None, fit_intercept=True,\n",
       "              loss='hinge', n_iter=5, n_jobs=1, random_state=None,\n",
       "              shuffle=True, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf = PassiveAggressiveClassifier(C=0.1)\n",
    "best_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./vectorizers/BestVectorizer.pkl', 'wb') as vectorizer_file:\n",
    "    _pickle.dump(best_vectorizer, vectorizer_file)\n",
    "    \n",
    "with open('./classifiers/BestClassifier.pkl', 'wb') as clf_file:\n",
    "    _pickle.dump(best_clf, clf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./classifiers/BestClassifier.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joblib.dump(best_vectorizer, \"./vectorizers/BestVectorizer.pkl\")\n",
    "#joblib.dump(best_clf, \"./classifiers/BestClassifier.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SentimentAnalyzer():\n",
    "    def __init__(self):\n",
    "        self.model = joblib.load('./classifiers/BestClassifier.pkl')\n",
    "        self.vectorizer = joblib.load(\"./vectorizers/BestVectorizer.pkl\")\n",
    "        vocabulary = joblib.load('./vectorizers/best_vectorizer_vocabulary.pkl')\n",
    "        self.vectorizer.vocabulary_ = vocabulary\n",
    "        self.classes_dict = {0: \"awful\", 1: \"very negative\", 2: \"negative\",\n",
    "                             3: \"satisfied\", 4: \"good\", 5: \"great\", -1: \"prediction error\"}\n",
    "        self.can_predict_proba = 'predict_proba' in dir(self.model)\n",
    "\n",
    "    def predict_text(self, text):\n",
    "        try:\n",
    "            vectorized = self.vectorizer.transform([text])\n",
    "            return self.model.predict(vectorized)[0], \\\n",
    "                   (self.model.predict_proba(vectorized)[0].max() if self.can_predict_proba else '')\n",
    "        except:\n",
    "            print('prediction error')\n",
    "            return -1, 0.8\n",
    "\n",
    "    def predict_list(self, list_of_texts):\n",
    "        try:\n",
    "            vectorized = self.vectorizer.transform(list_of_texts)\n",
    "            return self.model.predict(vectorized), \\\n",
    "                   (self.model.predict_proba(vectorized) if self.can_predict_proba else '')\n",
    "        except:\n",
    "            print('prediction error')\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_probability_words(probability):\n",
    "        if probability < 0.55:\n",
    "            return \"neutral or uncertain\"\n",
    "        if probability < 0.7:\n",
    "            return \"probably\"\n",
    "        if probability > 0.95:\n",
    "            return \"certain\"\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    def get_prediction_message(self, text):\n",
    "        prediction = self.predict_text(text)\n",
    "        class_prediction = prediction[0]\n",
    "        prediction_probability = prediction[1]\n",
    "        words = (self.get_probability_words(prediction_probability) if self.can_predict_proba \\\n",
    "                                            else 'I see.. The mark is')\n",
    "        return words + ' ' + str(class_prediction) + ' (' + self.classes_dict[class_prediction] + ')'\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        response = [input_data]\n",
    "        response = self.vectorizer.transform(response)\n",
    "        return self.model.predict(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "respalizer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I see.. The mark is 1 (very negative)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = 'Сбербанка'\n",
    "respalizer.get_prediction_message(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: \n",
    "- do the **oversampling**\n",
    "- parameter **tuning** for ensembles\n",
    "- give it a try to **reduce space dimension (PCA?)** and try **Lasso feature_selection** (http://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "- make the **GridSearch** for classifiers (might require an additional resources, such as Amazon machine, to end in an adequate time)\n",
    "- try **LDA** for feature transformation\n",
    "- **gensim Word2Vec**\n",
    "- use **other metrics** to measure classifiaction quality\n",
    "- Same task, but stated as a **Regression**\n",
    "- do some **visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
